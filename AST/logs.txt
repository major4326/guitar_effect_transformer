Step: 10, {'loss': 0.7034, 'learning_rate': 2.95724402084354e-06, 'epoch': 0.01}
Step: 20, {'loss': 0.6355, 'learning_rate': 2.9144880416870796e-06, 'epoch': 0.03}
Step: 30, {'loss': 0.6097, 'learning_rate': 2.8717320625306196e-06, 'epoch': 0.04}
Step: 40, {'loss': 0.6077, 'learning_rate': 2.8289760833741595e-06, 'epoch': 0.06}
Step: 50, {'loss': 0.5742, 'learning_rate': 2.786220104217699e-06, 'epoch': 0.07}
Step: 60, {'loss': 0.5718, 'learning_rate': 2.743464125061239e-06, 'epoch': 0.09}
Step: 70, {'loss': 0.5584, 'learning_rate': 2.700708145904779e-06, 'epoch': 0.1}
Step: 80, {'loss': 0.5496, 'learning_rate': 2.657952166748319e-06, 'epoch': 0.11}
Step: 90, {'loss': 0.5282, 'learning_rate': 2.6151961875918586e-06, 'epoch': 0.13}
Step: 100, {'loss': 0.5139, 'learning_rate': 2.5724402084353985e-06, 'epoch': 0.14}
Step: 110, {'loss': 0.5033, 'learning_rate': 2.529684229278938e-06, 'epoch': 0.16}
Step: 120, {'loss': 0.4843, 'learning_rate': 2.486928250122478e-06, 'epoch': 0.17}
Step: 130, {'loss': 0.4826, 'learning_rate': 2.444172270966018e-06, 'epoch': 0.19}
Step: 140, {'loss': 0.4644, 'learning_rate': 2.401416291809558e-06, 'epoch': 0.2}
Step: 150, {'loss': 0.452, 'learning_rate': 2.3586603126530975e-06, 'epoch': 0.21}
Step: 160, {'loss': 0.447, 'learning_rate': 2.3159043334966375e-06, 'epoch': 0.23}
Step: 170, {'loss': 0.4321, 'learning_rate': 2.2731483543401775e-06, 'epoch': 0.24}
Step: 180, {'loss': 0.421, 'learning_rate': 2.230392375183717e-06, 'epoch': 0.26}
Step: 190, {'loss': 0.4304, 'learning_rate': 2.187636396027257e-06, 'epoch': 0.27}
Step: 200, {'loss': 0.4122, 'learning_rate': 2.1448804168707966e-06, 'epoch': 0.29}
Step: 210, {'loss': 0.399, 'learning_rate': 2.102124437714337e-06, 'epoch': 0.3}
Step: 220, {'loss': 0.393, 'learning_rate': 2.0593684585578765e-06, 'epoch': 0.31}
Step: 230, {'loss': 0.3973, 'learning_rate': 2.0166124794014165e-06, 'epoch': 0.33}
Step: 240, {'loss': 0.3817, 'learning_rate': 1.973856500244956e-06, 'epoch': 0.34}
Step: 250, {'loss': 0.3941, 'learning_rate': 1.931100521088496e-06, 'epoch': 0.36}
Step: 260, {'loss': 0.3798, 'learning_rate': 1.888344541932036e-06, 'epoch': 0.37}
Step: 270, {'loss': 0.386, 'learning_rate': 1.8455885627755757e-06, 'epoch': 0.38}
Step: 280, {'loss': 0.3641, 'learning_rate': 1.8028325836191155e-06, 'epoch': 0.4}
Step: 290, {'loss': 0.3609, 'learning_rate': 1.7600766044626553e-06, 'epoch': 0.41}
Step: 300, {'loss': 0.3673, 'learning_rate': 1.717320625306195e-06, 'epoch': 0.43}
Step: 310, {'loss': 0.3607, 'learning_rate': 1.6745646461497352e-06, 'epoch': 0.44}
Step: 320, {'loss': 0.3583, 'learning_rate': 1.631808666993275e-06, 'epoch': 0.46}
Step: 330, {'loss': 0.3515, 'learning_rate': 1.5890526878368147e-06, 'epoch': 0.47}
Step: 340, {'loss': 0.3503, 'learning_rate': 1.5462967086803545e-06, 'epoch': 0.48}
Step: 350, {'loss': 0.3445, 'learning_rate': 1.5035407295238945e-06, 'epoch': 0.5}
Step: 360, {'loss': 0.3405, 'learning_rate': 1.4607847503674342e-06, 'epoch': 0.51}
Step: 370, {'loss': 0.3398, 'learning_rate': 1.418028771210974e-06, 'epoch': 0.53}
Step: 380, {'loss': 0.3349, 'learning_rate': 1.3752727920545137e-06, 'epoch': 0.54}
Step: 390, {'loss': 0.342, 'learning_rate': 1.3325168128980537e-06, 'epoch': 0.56}
Step: 400, {'loss': 0.3384, 'learning_rate': 1.2897608337415935e-06, 'epoch': 0.57}
Step: 410, {'loss': 0.3273, 'learning_rate': 1.2470048545851334e-06, 'epoch': 0.58}
Step: 420, {'loss': 0.3355, 'learning_rate': 1.2042488754286732e-06, 'epoch': 0.6}
Step: 430, {'loss': 0.3389, 'learning_rate': 1.1614928962722132e-06, 'epoch': 0.61}
Step: 440, {'loss': 0.339, 'learning_rate': 1.118736917115753e-06, 'epoch': 0.63}
Step: 450, {'loss': 0.3286, 'learning_rate': 1.0759809379592927e-06, 'epoch': 0.64}
Step: 460, {'loss': 0.3194, 'learning_rate': 1.0332249588028327e-06, 'epoch': 0.66}
Step: 470, {'loss': 0.3246, 'learning_rate': 9.904689796463724e-07, 'epoch': 0.67}
Step: 480, {'loss': 0.3156, 'learning_rate': 9.477130004899123e-07, 'epoch': 0.68}
Step: 490, {'loss': 0.3255, 'learning_rate': 9.049570213334521e-07, 'epoch': 0.7}
Step: 500, {'loss': 0.3033, 'learning_rate': 8.62201042176992e-07, 'epoch': 0.71}
Step: 510, {'loss': 0.307, 'learning_rate': 8.194450630205318e-07, 'epoch': 0.73}
Step: 520, {'loss': 0.3202, 'learning_rate': 7.766890838640716e-07, 'epoch': 0.74}
Step: 530, {'loss': 0.3149, 'learning_rate': 7.339331047076115e-07, 'epoch': 0.76}
Step: 540, {'loss': 0.3184, 'learning_rate': 6.911771255511513e-07, 'epoch': 0.77}
Step: 550, {'loss': 0.325, 'learning_rate': 6.484211463946912e-07, 'epoch': 0.78}
Step: 560, {'loss': 0.3106, 'learning_rate': 6.056651672382309e-07, 'epoch': 0.8}
Step: 570, {'loss': 0.3101, 'learning_rate': 5.629091880817708e-07, 'epoch': 0.81}
Step: 580, {'loss': 0.3133, 'learning_rate': 5.201532089253106e-07, 'epoch': 0.83}
Step: 590, {'loss': 0.3099, 'learning_rate': 4.773972297688505e-07, 'epoch': 0.84}
Step: 600, {'loss': 0.3058, 'learning_rate': 4.3464125061239033e-07, 'epoch': 0.86}
Step: 610, {'loss': 0.3041, 'learning_rate': 3.918852714559302e-07, 'epoch': 0.87}
Step: 620, {'loss': 0.3099, 'learning_rate': 3.4912929229947e-07, 'epoch': 0.88}
Step: 630, {'loss': 0.3099, 'learning_rate': 3.063733131430099e-07, 'epoch': 0.9}
Step: 640, {'loss': 0.3043, 'learning_rate': 2.636173339865497e-07, 'epoch': 0.91}
Step: 650, {'loss': 0.2958, 'learning_rate': 2.208613548300895e-07, 'epoch': 0.93}
Step: 660, {'loss': 0.2948, 'learning_rate': 1.7810537567362937e-07, 'epoch': 0.94}
Step: 670, {'loss': 0.3072, 'learning_rate': 1.353493965171692e-07, 'epoch': 0.95}
Step: 680, {'loss': 0.3021, 'learning_rate': 9.259341736070904e-08, 'epoch': 0.97}
Step: 690, {'loss': 0.3141, 'learning_rate': 4.983743820424888e-08, 'epoch': 0.98}
Step: 700, {'loss': 0.3007, 'learning_rate': 7.081459047788714e-09, 'epoch': 1.0}
Step: 702, {'train_runtime': 3075.9824, 'train_samples_per_second': 3.422, 'train_steps_per_second': 0.228, 'total_flos': 7.139061406000742e+17, 'train_loss': 0.3869710726785524, 'epoch': 1.0}
Step: 702, {'eval_loss': 0.406704306602478, 'eval_accuracy': 0.04, 'eval_f1': 0.760242251514072, 'eval_runtime': 31.3006, 'eval_samples_per_second': 9.584, 'eval_steps_per_second': 1.214, 'epoch': 1.0}
Step: 1, {'loss': 0.7502, 'learning_rate': 2.9e-06, 'epoch': 0.03}
Step: 2, {'loss': 0.7524, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.07}
Step: 3, {'loss': 0.7155, 'learning_rate': 2.7e-06, 'epoch': 0.1}
Step: 4, {'loss': 0.707, 'learning_rate': 2.6e-06, 'epoch': 0.13}
Step: 5, {'loss': 0.6803, 'learning_rate': 2.5e-06, 'epoch': 0.17}
Step: 6, {'loss': 0.6798, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.2}
Step: 7, {'loss': 0.6758, 'learning_rate': 2.3000000000000004e-06, 'epoch': 0.23}
Step: 8, {'loss': 0.6741, 'learning_rate': 2.1999999999999997e-06, 'epoch': 0.27}
Step: 9, {'loss': 0.6375, 'learning_rate': 2.1e-06, 'epoch': 0.3}
Step: 10, {'loss': 0.6819, 'learning_rate': 2e-06, 'epoch': 0.33}
Step: 11, {'loss': 0.6648, 'learning_rate': 1.9e-06, 'epoch': 0.37}
Step: 12, {'loss': 0.6694, 'learning_rate': 1.8e-06, 'epoch': 0.4}
Step: 13, {'loss': 0.6492, 'learning_rate': 1.7e-06, 'epoch': 0.43}
Step: 14, {'loss': 0.6416, 'learning_rate': 1.6e-06, 'epoch': 0.47}
Step: 15, {'loss': 0.6456, 'learning_rate': 1.5e-06, 'epoch': 0.5}
Step: 16, {'loss': 0.6457, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.53}
Step: 17, {'loss': 0.6561, 'learning_rate': 1.3e-06, 'epoch': 0.57}
Step: 18, {'loss': 0.6338, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.6}
Step: 19, {'loss': 0.6331, 'learning_rate': 1.0999999999999998e-06, 'epoch': 0.63}
Step: 20, {'loss': 0.6393, 'learning_rate': 1e-06, 'epoch': 0.67}
Step: 21, {'loss': 0.622, 'learning_rate': 9e-07, 'epoch': 0.7}
Step: 22, {'loss': 0.6322, 'learning_rate': 8e-07, 'epoch': 0.73}
Step: 23, {'loss': 0.6263, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.77}
Step: 24, {'loss': 0.6079, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.8}
Step: 25, {'loss': 0.6103, 'learning_rate': 5e-07, 'epoch': 0.83}
Step: 26, {'loss': 0.6422, 'learning_rate': 4e-07, 'epoch': 0.87}
Step: 27, {'loss': 0.6232, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.9}
Step: 28, {'loss': 0.6358, 'learning_rate': 2e-07, 'epoch': 0.93}
Step: 29, {'loss': 0.649, 'learning_rate': 1e-07, 'epoch': 0.97}
Step: 30, {'loss': 0.5956, 'learning_rate': 0.0, 'epoch': 1.0}
Step: 30, {'train_runtime': 129.9189, 'train_samples_per_second': 3.464, 'train_steps_per_second': 0.231, 'total_flos': 3.05088094273536e+16, 'train_loss': 0.6559178789456686, 'epoch': 1.0}
Step: 30, {'eval_loss': 0.6251347661018372, 'eval_accuracy': 0.0033333333333333335, 'eval_f1': 0.3509127789046653, 'eval_runtime': 31.2122, 'eval_samples_per_second': 9.612, 'eval_steps_per_second': 1.217, 'epoch': 1.0}
Step: 1, {'loss': 0.2521, 'learning_rate': 2.9957203994293868e-06, 'epoch': 0.0}
Step: 2, {'loss': 0.3128, 'learning_rate': 2.9914407988587735e-06, 'epoch': 0.0}
Step: 3, {'loss': 0.3179, 'learning_rate': 2.9871611982881598e-06, 'epoch': 0.0}
Step: 4, {'loss': 0.3483, 'learning_rate': 2.9828815977175465e-06, 'epoch': 0.01}
Step: 5, {'loss': 0.3131, 'learning_rate': 2.978601997146933e-06, 'epoch': 0.01}
Step: 6, {'loss': 0.3087, 'learning_rate': 2.97432239657632e-06, 'epoch': 0.01}
Step: 7, {'loss': 0.2918, 'learning_rate': 2.9700427960057062e-06, 'epoch': 0.01}
Step: 8, {'loss': 0.3133, 'learning_rate': 2.965763195435093e-06, 'epoch': 0.01}
Step: 9, {'loss': 0.2784, 'learning_rate': 2.9614835948644792e-06, 'epoch': 0.01}
Step: 10, {'loss': 0.2796, 'learning_rate': 2.957203994293866e-06, 'epoch': 0.01}
Step: 11, {'loss': 0.3084, 'learning_rate': 2.9529243937232526e-06, 'epoch': 0.02}
Step: 12, {'loss': 0.2953, 'learning_rate': 2.948644793152639e-06, 'epoch': 0.02}
Step: 13, {'loss': 0.3048, 'learning_rate': 2.9443651925820256e-06, 'epoch': 0.02}
Step: 14, {'loss': 0.2962, 'learning_rate': 2.9400855920114123e-06, 'epoch': 0.02}
Step: 15, {'loss': 0.2948, 'learning_rate': 2.935805991440799e-06, 'epoch': 0.02}
Step: 16, {'loss': 0.2977, 'learning_rate': 2.9315263908701853e-06, 'epoch': 0.02}
Step: 17, {'loss': 0.3462, 'learning_rate': 2.927246790299572e-06, 'epoch': 0.02}
Step: 18, {'loss': 0.3603, 'learning_rate': 2.9229671897289588e-06, 'epoch': 0.03}
Step: 19, {'loss': 0.2864, 'learning_rate': 2.9186875891583455e-06, 'epoch': 0.03}
Step: 20, {'loss': 0.2771, 'learning_rate': 2.9144079885877318e-06, 'epoch': 0.03}
Step: 21, {'loss': 0.3182, 'learning_rate': 2.9101283880171185e-06, 'epoch': 0.03}
Step: 22, {'loss': 0.3247, 'learning_rate': 2.905848787446505e-06, 'epoch': 0.03}
Step: 23, {'loss': 0.2969, 'learning_rate': 2.9015691868758915e-06, 'epoch': 0.03}
Step: 24, {'loss': 0.2799, 'learning_rate': 2.897289586305278e-06, 'epoch': 0.03}
Step: 25, {'loss': 0.3018, 'learning_rate': 2.8930099857346645e-06, 'epoch': 0.04}
Step: 26, {'loss': 0.2775, 'learning_rate': 2.8887303851640516e-06, 'epoch': 0.04}
Step: 27, {'loss': 0.303, 'learning_rate': 2.884450784593438e-06, 'epoch': 0.04}
Step: 28, {'loss': 0.2978, 'learning_rate': 2.8801711840228246e-06, 'epoch': 0.04}
Step: 29, {'loss': 0.2844, 'learning_rate': 2.875891583452211e-06, 'epoch': 0.04}
Step: 30, {'loss': 0.2796, 'learning_rate': 2.871611982881598e-06, 'epoch': 0.04}
Step: 31, {'loss': 0.2603, 'learning_rate': 2.8673323823109843e-06, 'epoch': 0.04}
Step: 32, {'loss': 0.3115, 'learning_rate': 2.863052781740371e-06, 'epoch': 0.05}
Step: 33, {'loss': 0.2736, 'learning_rate': 2.8587731811697578e-06, 'epoch': 0.05}
Step: 34, {'loss': 0.3125, 'learning_rate': 2.854493580599144e-06, 'epoch': 0.05}
Step: 35, {'loss': 0.3042, 'learning_rate': 2.8502139800285308e-06, 'epoch': 0.05}
Step: 36, {'loss': 0.2855, 'learning_rate': 2.845934379457917e-06, 'epoch': 0.05}
Step: 37, {'loss': 0.2852, 'learning_rate': 2.841654778887304e-06, 'epoch': 0.05}
Step: 38, {'loss': 0.3164, 'learning_rate': 2.8373751783166905e-06, 'epoch': 0.05}
Step: 39, {'loss': 0.3338, 'learning_rate': 2.833095577746077e-06, 'epoch': 0.06}
Step: 40, {'loss': 0.3749, 'learning_rate': 2.8288159771754635e-06, 'epoch': 0.06}
Step: 41, {'loss': 0.2983, 'learning_rate': 2.8245363766048506e-06, 'epoch': 0.06}
Step: 42, {'loss': 0.3427, 'learning_rate': 2.820256776034237e-06, 'epoch': 0.06}
Step: 43, {'loss': 0.2944, 'learning_rate': 2.8159771754636236e-06, 'epoch': 0.06}
Step: 44, {'loss': 0.2764, 'learning_rate': 2.81169757489301e-06, 'epoch': 0.06}
Step: 45, {'loss': 0.3058, 'learning_rate': 2.8074179743223966e-06, 'epoch': 0.06}
Step: 46, {'loss': 0.2718, 'learning_rate': 2.8031383737517833e-06, 'epoch': 0.07}
Step: 47, {'loss': 0.2497, 'learning_rate': 2.7988587731811696e-06, 'epoch': 0.07}
Step: 48, {'loss': 0.2868, 'learning_rate': 2.7945791726105563e-06, 'epoch': 0.07}
Step: 49, {'loss': 0.3208, 'learning_rate': 2.790299572039943e-06, 'epoch': 0.07}
Step: 50, {'loss': 0.3066, 'learning_rate': 2.7860199714693297e-06, 'epoch': 0.07}
Step: 51, {'loss': 0.2872, 'learning_rate': 2.781740370898716e-06, 'epoch': 0.07}
Step: 52, {'loss': 0.2799, 'learning_rate': 2.7774607703281027e-06, 'epoch': 0.07}
Step: 53, {'loss': 0.3012, 'learning_rate': 2.7731811697574895e-06, 'epoch': 0.08}
Step: 54, {'loss': 0.3152, 'learning_rate': 2.768901569186876e-06, 'epoch': 0.08}
Step: 55, {'loss': 0.3326, 'learning_rate': 2.7646219686162625e-06, 'epoch': 0.08}
Step: 56, {'loss': 0.28, 'learning_rate': 2.760342368045649e-06, 'epoch': 0.08}
Step: 57, {'loss': 0.2502, 'learning_rate': 2.756062767475036e-06, 'epoch': 0.08}
Step: 58, {'loss': 0.2484, 'learning_rate': 2.751783166904422e-06, 'epoch': 0.08}
Step: 59, {'loss': 0.2902, 'learning_rate': 2.747503566333809e-06, 'epoch': 0.08}
Step: 60, {'loss': 0.2673, 'learning_rate': 2.743223965763195e-06, 'epoch': 0.09}
Step: 61, {'loss': 0.2517, 'learning_rate': 2.7389443651925823e-06, 'epoch': 0.09}
Step: 62, {'loss': 0.2918, 'learning_rate': 2.7346647646219686e-06, 'epoch': 0.09}
Step: 63, {'loss': 0.2959, 'learning_rate': 2.7303851640513553e-06, 'epoch': 0.09}
Step: 64, {'loss': 0.3241, 'learning_rate': 2.726105563480742e-06, 'epoch': 0.09}
Step: 65, {'loss': 0.2856, 'learning_rate': 2.7218259629101287e-06, 'epoch': 0.09}
Step: 66, {'loss': 0.3101, 'learning_rate': 2.717546362339515e-06, 'epoch': 0.09}
Step: 67, {'loss': 0.2649, 'learning_rate': 2.7132667617689017e-06, 'epoch': 0.1}
Step: 68, {'loss': 0.2773, 'learning_rate': 2.7089871611982884e-06, 'epoch': 0.1}
Step: 69, {'loss': 0.2939, 'learning_rate': 2.7047075606276747e-06, 'epoch': 0.1}
Step: 70, {'loss': 0.2574, 'learning_rate': 2.7004279600570614e-06, 'epoch': 0.1}
Step: 71, {'loss': 0.2732, 'learning_rate': 2.6961483594864477e-06, 'epoch': 0.1}
Step: 72, {'loss': 0.3006, 'learning_rate': 2.691868758915835e-06, 'epoch': 0.1}
Step: 73, {'loss': 0.2683, 'learning_rate': 2.687589158345221e-06, 'epoch': 0.1}
Step: 74, {'loss': 0.2936, 'learning_rate': 2.683309557774608e-06, 'epoch': 0.11}
Step: 75, {'loss': 0.2978, 'learning_rate': 2.679029957203994e-06, 'epoch': 0.11}
Step: 76, {'loss': 0.3496, 'learning_rate': 2.6747503566333813e-06, 'epoch': 0.11}
Step: 77, {'loss': 0.299, 'learning_rate': 2.6704707560627676e-06, 'epoch': 0.11}
Step: 78, {'loss': 0.2783, 'learning_rate': 2.6661911554921543e-06, 'epoch': 0.11}
Step: 79, {'loss': 0.2794, 'learning_rate': 2.6619115549215406e-06, 'epoch': 0.11}
Step: 80, {'loss': 0.2951, 'learning_rate': 2.6576319543509273e-06, 'epoch': 0.11}
Step: 81, {'loss': 0.2973, 'learning_rate': 2.653352353780314e-06, 'epoch': 0.12}
Step: 82, {'loss': 0.2693, 'learning_rate': 2.6490727532097003e-06, 'epoch': 0.12}
Step: 83, {'loss': 0.3231, 'learning_rate': 2.644793152639087e-06, 'epoch': 0.12}
Step: 84, {'loss': 0.2856, 'learning_rate': 2.6405135520684737e-06, 'epoch': 0.12}
Step: 85, {'loss': 0.2534, 'learning_rate': 2.6362339514978604e-06, 'epoch': 0.12}
Step: 86, {'loss': 0.2835, 'learning_rate': 2.6319543509272467e-06, 'epoch': 0.12}
Step: 87, {'loss': 0.2817, 'learning_rate': 2.6276747503566334e-06, 'epoch': 0.12}
Step: 88, {'loss': 0.3021, 'learning_rate': 2.62339514978602e-06, 'epoch': 0.13}
Step: 89, {'loss': 0.2963, 'learning_rate': 2.6191155492154064e-06, 'epoch': 0.13}
Step: 90, {'loss': 0.2942, 'learning_rate': 2.614835948644793e-06, 'epoch': 0.13}
Step: 91, {'loss': 0.289, 'learning_rate': 2.61055634807418e-06, 'epoch': 0.13}
Step: 92, {'loss': 0.2669, 'learning_rate': 2.6062767475035666e-06, 'epoch': 0.13}
Step: 93, {'loss': 0.2389, 'learning_rate': 2.601997146932953e-06, 'epoch': 0.13}
Step: 94, {'loss': 0.2756, 'learning_rate': 2.5977175463623396e-06, 'epoch': 0.13}
Step: 95, {'loss': 0.2928, 'learning_rate': 2.5934379457917263e-06, 'epoch': 0.14}
Step: 96, {'loss': 0.2559, 'learning_rate': 2.589158345221113e-06, 'epoch': 0.14}
Step: 97, {'loss': 0.2598, 'learning_rate': 2.5848787446504993e-06, 'epoch': 0.14}
Step: 98, {'loss': 0.2647, 'learning_rate': 2.580599144079886e-06, 'epoch': 0.14}
Step: 99, {'loss': 0.2919, 'learning_rate': 2.5763195435092727e-06, 'epoch': 0.14}
Step: 100, {'loss': 0.284, 'learning_rate': 2.572039942938659e-06, 'epoch': 0.14}
Step: 101, {'loss': 0.2536, 'learning_rate': 2.5677603423680457e-06, 'epoch': 0.14}
Step: 102, {'loss': 0.2718, 'learning_rate': 2.563480741797432e-06, 'epoch': 0.15}
Step: 103, {'loss': 0.251, 'learning_rate': 2.559201141226819e-06, 'epoch': 0.15}
Step: 104, {'loss': 0.2692, 'learning_rate': 2.5549215406562054e-06, 'epoch': 0.15}
Step: 105, {'loss': 0.2844, 'learning_rate': 2.550641940085592e-06, 'epoch': 0.15}
Step: 106, {'loss': 0.2762, 'learning_rate': 2.5463623395149784e-06, 'epoch': 0.15}
Step: 107, {'loss': 0.2665, 'learning_rate': 2.5420827389443655e-06, 'epoch': 0.15}
Step: 108, {'loss': 0.286, 'learning_rate': 2.537803138373752e-06, 'epoch': 0.15}
Step: 109, {'loss': 0.3154, 'learning_rate': 2.5335235378031385e-06, 'epoch': 0.16}
Step: 110, {'loss': 0.2475, 'learning_rate': 2.529243937232525e-06, 'epoch': 0.16}
Step: 111, {'loss': 0.2742, 'learning_rate': 2.5249643366619115e-06, 'epoch': 0.16}
Step: 112, {'loss': 0.278, 'learning_rate': 2.5206847360912983e-06, 'epoch': 0.16}
Step: 113, {'loss': 0.2819, 'learning_rate': 2.5164051355206845e-06, 'epoch': 0.16}
Step: 114, {'loss': 0.2647, 'learning_rate': 2.5121255349500713e-06, 'epoch': 0.16}
Step: 115, {'loss': 0.2489, 'learning_rate': 2.507845934379458e-06, 'epoch': 0.16}
Step: 116, {'loss': 0.2714, 'learning_rate': 2.5035663338088447e-06, 'epoch': 0.17}
Step: 117, {'loss': 0.2827, 'learning_rate': 2.499286733238231e-06, 'epoch': 0.17}
Step: 118, {'loss': 0.2676, 'learning_rate': 2.4950071326676177e-06, 'epoch': 0.17}
Step: 119, {'loss': 0.2772, 'learning_rate': 2.4907275320970044e-06, 'epoch': 0.17}
Step: 120, {'loss': 0.2861, 'learning_rate': 2.486447931526391e-06, 'epoch': 0.17}
Step: 121, {'loss': 0.2604, 'learning_rate': 2.4821683309557774e-06, 'epoch': 0.17}
Step: 122, {'loss': 0.2623, 'learning_rate': 2.477888730385164e-06, 'epoch': 0.17}
Step: 123, {'loss': 0.2518, 'learning_rate': 2.473609129814551e-06, 'epoch': 0.18}
Step: 124, {'loss': 0.2675, 'learning_rate': 2.469329529243937e-06, 'epoch': 0.18}
Step: 125, {'loss': 0.261, 'learning_rate': 2.465049928673324e-06, 'epoch': 0.18}
Step: 126, {'loss': 0.2487, 'learning_rate': 2.4607703281027105e-06, 'epoch': 0.18}
Step: 127, {'loss': 0.2516, 'learning_rate': 2.4564907275320972e-06, 'epoch': 0.18}
Step: 128, {'loss': 0.2552, 'learning_rate': 2.4522111269614835e-06, 'epoch': 0.18}
Step: 129, {'loss': 0.2959, 'learning_rate': 2.4479315263908702e-06, 'epoch': 0.18}
Step: 130, {'loss': 0.3095, 'learning_rate': 2.443651925820257e-06, 'epoch': 0.19}
Step: 131, {'loss': 0.2295, 'learning_rate': 2.4393723252496437e-06, 'epoch': 0.19}
Step: 132, {'loss': 0.2511, 'learning_rate': 2.43509272467903e-06, 'epoch': 0.19}
Step: 133, {'loss': 0.2712, 'learning_rate': 2.4308131241084167e-06, 'epoch': 0.19}
Step: 134, {'loss': 0.2224, 'learning_rate': 2.4265335235378034e-06, 'epoch': 0.19}
Step: 135, {'loss': 0.2501, 'learning_rate': 2.4222539229671897e-06, 'epoch': 0.19}
Step: 136, {'loss': 0.2536, 'learning_rate': 2.4179743223965764e-06, 'epoch': 0.19}
Step: 137, {'loss': 0.2373, 'learning_rate': 2.4136947218259627e-06, 'epoch': 0.2}
Step: 138, {'loss': 0.287, 'learning_rate': 2.40941512125535e-06, 'epoch': 0.2}
Step: 139, {'loss': 0.2441, 'learning_rate': 2.405135520684736e-06, 'epoch': 0.2}
Step: 140, {'loss': 0.2668, 'learning_rate': 2.400855920114123e-06, 'epoch': 0.2}
Step: 141, {'loss': 0.258, 'learning_rate': 2.396576319543509e-06, 'epoch': 0.2}
Step: 142, {'loss': 0.2696, 'learning_rate': 2.3922967189728962e-06, 'epoch': 0.2}
Step: 143, {'loss': 0.2362, 'learning_rate': 2.3880171184022825e-06, 'epoch': 0.2}
Step: 144, {'loss': 0.2515, 'learning_rate': 2.3837375178316692e-06, 'epoch': 0.21}
Step: 145, {'loss': 0.294, 'learning_rate': 2.3794579172610555e-06, 'epoch': 0.21}
Step: 146, {'loss': 0.2679, 'learning_rate': 2.3751783166904422e-06, 'epoch': 0.21}
Step: 147, {'loss': 0.2951, 'learning_rate': 2.370898716119829e-06, 'epoch': 0.21}
Step: 148, {'loss': 0.2719, 'learning_rate': 2.3666191155492152e-06, 'epoch': 0.21}
Step: 149, {'loss': 0.2594, 'learning_rate': 2.362339514978602e-06, 'epoch': 0.21}
Step: 150, {'loss': 0.2185, 'learning_rate': 2.3580599144079887e-06, 'epoch': 0.21}
Step: 151, {'loss': 0.3096, 'learning_rate': 2.3537803138373754e-06, 'epoch': 0.22}
Step: 152, {'loss': 0.2543, 'learning_rate': 2.3495007132667617e-06, 'epoch': 0.22}
Step: 153, {'loss': 0.265, 'learning_rate': 2.3452211126961488e-06, 'epoch': 0.22}
Step: 154, {'loss': 0.2597, 'learning_rate': 2.340941512125535e-06, 'epoch': 0.22}
Step: 155, {'loss': 0.2769, 'learning_rate': 2.3366619115549218e-06, 'epoch': 0.22}
Step: 156, {'loss': 0.2414, 'learning_rate': 2.332382310984308e-06, 'epoch': 0.22}
Step: 157, {'loss': 0.2736, 'learning_rate': 2.3281027104136948e-06, 'epoch': 0.22}
Step: 158, {'loss': 0.2662, 'learning_rate': 2.3238231098430815e-06, 'epoch': 0.23}
Step: 159, {'loss': 0.263, 'learning_rate': 2.319543509272468e-06, 'epoch': 0.23}
Step: 160, {'loss': 0.2275, 'learning_rate': 2.3152639087018545e-06, 'epoch': 0.23}
Step: 161, {'loss': 0.26, 'learning_rate': 2.3109843081312412e-06, 'epoch': 0.23}
Step: 162, {'loss': 0.2506, 'learning_rate': 2.306704707560628e-06, 'epoch': 0.23}
Step: 163, {'loss': 0.2481, 'learning_rate': 2.3024251069900142e-06, 'epoch': 0.23}
Step: 164, {'loss': 0.2377, 'learning_rate': 2.298145506419401e-06, 'epoch': 0.23}
Step: 165, {'loss': 0.2826, 'learning_rate': 2.2938659058487876e-06, 'epoch': 0.24}
Step: 166, {'loss': 0.2511, 'learning_rate': 2.2895863052781743e-06, 'epoch': 0.24}
Step: 167, {'loss': 0.2177, 'learning_rate': 2.2853067047075606e-06, 'epoch': 0.24}
Step: 168, {'loss': 0.2759, 'learning_rate': 2.2810271041369473e-06, 'epoch': 0.24}
Step: 169, {'loss': 0.3062, 'learning_rate': 2.276747503566334e-06, 'epoch': 0.24}
Step: 170, {'loss': 0.2274, 'learning_rate': 2.2724679029957203e-06, 'epoch': 0.24}
Step: 171, {'loss': 0.2681, 'learning_rate': 2.268188302425107e-06, 'epoch': 0.24}
Step: 172, {'loss': 0.2296, 'learning_rate': 2.2639087018544933e-06, 'epoch': 0.25}
Step: 173, {'loss': 0.2352, 'learning_rate': 2.2596291012838805e-06, 'epoch': 0.25}
Step: 174, {'loss': 0.2662, 'learning_rate': 2.2553495007132668e-06, 'epoch': 0.25}
Step: 175, {'loss': 0.3188, 'learning_rate': 2.2510699001426535e-06, 'epoch': 0.25}
Step: 176, {'loss': 0.2434, 'learning_rate': 2.2467902995720398e-06, 'epoch': 0.25}
Step: 177, {'loss': 0.2644, 'learning_rate': 2.2425106990014265e-06, 'epoch': 0.25}
Step: 178, {'loss': 0.2681, 'learning_rate': 2.238231098430813e-06, 'epoch': 0.25}
Step: 179, {'loss': 0.2476, 'learning_rate': 2.2339514978602e-06, 'epoch': 0.26}
Step: 180, {'loss': 0.2592, 'learning_rate': 2.229671897289586e-06, 'epoch': 0.26}
Step: 181, {'loss': 0.2928, 'learning_rate': 2.225392296718973e-06, 'epoch': 0.26}
Step: 182, {'loss': 0.2577, 'learning_rate': 2.2211126961483596e-06, 'epoch': 0.26}
Step: 183, {'loss': 0.246, 'learning_rate': 2.216833095577746e-06, 'epoch': 0.26}
Step: 184, {'loss': 0.284, 'learning_rate': 2.212553495007133e-06, 'epoch': 0.26}
Step: 185, {'loss': 0.2648, 'learning_rate': 2.2082738944365193e-06, 'epoch': 0.26}
Step: 186, {'loss': 0.2583, 'learning_rate': 2.203994293865906e-06, 'epoch': 0.27}
Step: 187, {'loss': 0.2297, 'learning_rate': 2.1997146932952923e-06, 'epoch': 0.27}
Step: 188, {'loss': 0.225, 'learning_rate': 2.195435092724679e-06, 'epoch': 0.27}
Step: 189, {'loss': 0.3146, 'learning_rate': 2.1911554921540658e-06, 'epoch': 0.27}
Step: 190, {'loss': 0.2408, 'learning_rate': 2.186875891583452e-06, 'epoch': 0.27}
Step: 191, {'loss': 0.2296, 'learning_rate': 2.1825962910128388e-06, 'epoch': 0.27}
Step: 192, {'loss': 0.2598, 'learning_rate': 2.1783166904422255e-06, 'epoch': 0.27}
Step: 193, {'loss': 0.2559, 'learning_rate': 2.174037089871612e-06, 'epoch': 0.28}
Step: 194, {'loss': 0.2623, 'learning_rate': 2.1697574893009985e-06, 'epoch': 0.28}
Step: 195, {'loss': 0.2541, 'learning_rate': 2.165477888730385e-06, 'epoch': 0.28}
Step: 196, {'loss': 0.2618, 'learning_rate': 2.161198288159772e-06, 'epoch': 0.28}
Step: 197, {'loss': 0.2255, 'learning_rate': 2.1569186875891586e-06, 'epoch': 0.28}
Step: 198, {'loss': 0.2839, 'learning_rate': 2.152639087018545e-06, 'epoch': 0.28}
Step: 199, {'loss': 0.2715, 'learning_rate': 2.1483594864479316e-06, 'epoch': 0.28}
Step: 200, {'loss': 0.2493, 'learning_rate': 2.1440798858773183e-06, 'epoch': 0.29}
Step: 201, {'loss': 0.2487, 'learning_rate': 2.1398002853067046e-06, 'epoch': 0.29}
Step: 202, {'loss': 0.2268, 'learning_rate': 2.1355206847360913e-06, 'epoch': 0.29}
Step: 203, {'loss': 0.2303, 'learning_rate': 2.1312410841654776e-06, 'epoch': 0.29}
Step: 204, {'loss': 0.2469, 'learning_rate': 2.1269614835948647e-06, 'epoch': 0.29}
Step: 205, {'loss': 0.2358, 'learning_rate': 2.122681883024251e-06, 'epoch': 0.29}
Step: 206, {'loss': 0.2513, 'learning_rate': 2.1184022824536377e-06, 'epoch': 0.29}
Step: 207, {'loss': 0.2473, 'learning_rate': 2.114122681883024e-06, 'epoch': 0.3}
Step: 208, {'loss': 0.2329, 'learning_rate': 2.109843081312411e-06, 'epoch': 0.3}
Step: 209, {'loss': 0.2794, 'learning_rate': 2.1055634807417975e-06, 'epoch': 0.3}
Step: 210, {'loss': 0.2805, 'learning_rate': 2.101283880171184e-06, 'epoch': 0.3}
Step: 211, {'loss': 0.2328, 'learning_rate': 2.0970042796005705e-06, 'epoch': 0.3}
Step: 212, {'loss': 0.2952, 'learning_rate': 2.092724679029957e-06, 'epoch': 0.3}
Step: 213, {'loss': 0.2894, 'learning_rate': 2.088445078459344e-06, 'epoch': 0.3}
Step: 214, {'loss': 0.2746, 'learning_rate': 2.08416547788873e-06, 'epoch': 0.31}
Step: 215, {'loss': 0.2563, 'learning_rate': 2.0798858773181173e-06, 'epoch': 0.31}
Step: 216, {'loss': 0.231, 'learning_rate': 2.0756062767475036e-06, 'epoch': 0.31}
Step: 217, {'loss': 0.2404, 'learning_rate': 2.0713266761768903e-06, 'epoch': 0.31}
Step: 218, {'loss': 0.24, 'learning_rate': 2.0670470756062766e-06, 'epoch': 0.31}
Step: 219, {'loss': 0.2284, 'learning_rate': 2.0627674750356637e-06, 'epoch': 0.31}
Step: 220, {'loss': 0.2537, 'learning_rate': 2.05848787446505e-06, 'epoch': 0.31}
Step: 221, {'loss': 0.2535, 'learning_rate': 2.0542082738944367e-06, 'epoch': 0.32}
Step: 222, {'loss': 0.2672, 'learning_rate': 2.049928673323823e-06, 'epoch': 0.32}
Step: 223, {'loss': 0.2891, 'learning_rate': 2.0456490727532097e-06, 'epoch': 0.32}
Step: 224, {'loss': 0.2696, 'learning_rate': 2.0413694721825964e-06, 'epoch': 0.32}
Step: 225, {'loss': 0.2468, 'learning_rate': 2.0370898716119827e-06, 'epoch': 0.32}
Step: 226, {'loss': 0.2538, 'learning_rate': 2.0328102710413694e-06, 'epoch': 0.32}
Step: 227, {'loss': 0.2603, 'learning_rate': 2.028530670470756e-06, 'epoch': 0.32}
Step: 228, {'loss': 0.2349, 'learning_rate': 2.024251069900143e-06, 'epoch': 0.33}
Step: 229, {'loss': 0.273, 'learning_rate': 2.019971469329529e-06, 'epoch': 0.33}
Step: 230, {'loss': 0.2605, 'learning_rate': 2.015691868758916e-06, 'epoch': 0.33}
Step: 231, {'loss': 0.2648, 'learning_rate': 2.0114122681883026e-06, 'epoch': 0.33}
Step: 232, {'loss': 0.2282, 'learning_rate': 2.0071326676176893e-06, 'epoch': 0.33}
Step: 233, {'loss': 0.2762, 'learning_rate': 2.0028530670470756e-06, 'epoch': 0.33}
Step: 234, {'loss': 0.2317, 'learning_rate': 1.9985734664764623e-06, 'epoch': 0.33}
Step: 235, {'loss': 0.2833, 'learning_rate': 1.994293865905849e-06, 'epoch': 0.34}
Step: 236, {'loss': 0.2722, 'learning_rate': 1.9900142653352353e-06, 'epoch': 0.34}
Step: 237, {'loss': 0.2456, 'learning_rate': 1.985734664764622e-06, 'epoch': 0.34}
Step: 238, {'loss': 0.2644, 'learning_rate': 1.9814550641940083e-06, 'epoch': 0.34}
Step: 239, {'loss': 0.241, 'learning_rate': 1.9771754636233954e-06, 'epoch': 0.34}
Step: 240, {'loss': 0.2523, 'learning_rate': 1.9728958630527817e-06, 'epoch': 0.34}
Step: 241, {'loss': 0.2717, 'learning_rate': 1.9686162624821684e-06, 'epoch': 0.34}
Step: 242, {'loss': 0.2102, 'learning_rate': 1.9643366619115547e-06, 'epoch': 0.35}
Step: 243, {'loss': 0.2754, 'learning_rate': 1.960057061340942e-06, 'epoch': 0.35}
Step: 244, {'loss': 0.2505, 'learning_rate': 1.955777460770328e-06, 'epoch': 0.35}
Step: 245, {'loss': 0.2261, 'learning_rate': 1.951497860199715e-06, 'epoch': 0.35}
Step: 246, {'loss': 0.2534, 'learning_rate': 1.9472182596291016e-06, 'epoch': 0.35}
Step: 247, {'loss': 0.2899, 'learning_rate': 1.942938659058488e-06, 'epoch': 0.35}
Step: 248, {'loss': 0.2584, 'learning_rate': 1.9386590584878746e-06, 'epoch': 0.35}
Step: 249, {'loss': 0.2493, 'learning_rate': 1.934379457917261e-06, 'epoch': 0.36}
Step: 250, {'loss': 0.2608, 'learning_rate': 1.930099857346648e-06, 'epoch': 0.36}
Step: 251, {'loss': 0.2808, 'learning_rate': 1.9258202567760343e-06, 'epoch': 0.36}
Step: 252, {'loss': 0.2581, 'learning_rate': 1.921540656205421e-06, 'epoch': 0.36}
Step: 253, {'loss': 0.2154, 'learning_rate': 1.9172610556348073e-06, 'epoch': 0.36}
Step: 254, {'loss': 0.2663, 'learning_rate': 1.912981455064194e-06, 'epoch': 0.36}
Step: 255, {'loss': 0.2676, 'learning_rate': 1.9087018544935807e-06, 'epoch': 0.36}
Step: 256, {'loss': 0.2551, 'learning_rate': 1.9044222539229672e-06, 'epoch': 0.37}
Step: 257, {'loss': 0.2518, 'learning_rate': 1.9001426533523537e-06, 'epoch': 0.37}
Step: 258, {'loss': 0.2198, 'learning_rate': 1.8958630527817406e-06, 'epoch': 0.37}
Step: 259, {'loss': 0.2358, 'learning_rate': 1.8915834522111271e-06, 'epoch': 0.37}
Step: 260, {'loss': 0.2652, 'learning_rate': 1.8873038516405136e-06, 'epoch': 0.37}
Step: 261, {'loss': 0.2955, 'learning_rate': 1.8830242510699001e-06, 'epoch': 0.37}
Step: 262, {'loss': 0.2639, 'learning_rate': 1.8787446504992868e-06, 'epoch': 0.37}
Step: 263, {'loss': 0.2397, 'learning_rate': 1.8744650499286733e-06, 'epoch': 0.38}
Step: 264, {'loss': 0.2854, 'learning_rate': 1.8701854493580598e-06, 'epoch': 0.38}
Step: 265, {'loss': 0.2922, 'learning_rate': 1.8659058487874463e-06, 'epoch': 0.38}
Step: 266, {'loss': 0.2603, 'learning_rate': 1.8616262482168333e-06, 'epoch': 0.38}
Step: 267, {'loss': 0.2213, 'learning_rate': 1.8573466476462198e-06, 'epoch': 0.38}
Step: 268, {'loss': 0.2384, 'learning_rate': 1.8530670470756063e-06, 'epoch': 0.38}
Step: 269, {'loss': 0.2517, 'learning_rate': 1.8487874465049928e-06, 'epoch': 0.38}
Step: 270, {'loss': 0.2204, 'learning_rate': 1.8445078459343797e-06, 'epoch': 0.39}
Step: 271, {'loss': 0.2375, 'learning_rate': 1.8402282453637662e-06, 'epoch': 0.39}
Step: 272, {'loss': 0.2442, 'learning_rate': 1.8359486447931527e-06, 'epoch': 0.39}
Step: 273, {'loss': 0.2357, 'learning_rate': 1.8316690442225392e-06, 'epoch': 0.39}
Step: 274, {'loss': 0.2441, 'learning_rate': 1.8273894436519259e-06, 'epoch': 0.39}
Step: 275, {'loss': 0.226, 'learning_rate': 1.8231098430813124e-06, 'epoch': 0.39}
Step: 276, {'loss': 0.2363, 'learning_rate': 1.818830242510699e-06, 'epoch': 0.39}
Step: 277, {'loss': 0.2635, 'learning_rate': 1.8145506419400858e-06, 'epoch': 0.4}
Step: 278, {'loss': 0.2213, 'learning_rate': 1.8102710413694723e-06, 'epoch': 0.4}
Step: 279, {'loss': 0.2512, 'learning_rate': 1.8059914407988588e-06, 'epoch': 0.4}
Step: 280, {'loss': 0.2433, 'learning_rate': 1.8017118402282453e-06, 'epoch': 0.4}
Step: 281, {'loss': 0.2497, 'learning_rate': 1.7974322396576322e-06, 'epoch': 0.4}
Step: 282, {'loss': 0.2183, 'learning_rate': 1.7931526390870187e-06, 'epoch': 0.4}
Step: 283, {'loss': 0.2292, 'learning_rate': 1.7888730385164052e-06, 'epoch': 0.4}
Step: 284, {'loss': 0.2653, 'learning_rate': 1.7845934379457917e-06, 'epoch': 0.41}
Step: 285, {'loss': 0.2502, 'learning_rate': 1.7803138373751785e-06, 'epoch': 0.41}
Step: 286, {'loss': 0.233, 'learning_rate': 1.776034236804565e-06, 'epoch': 0.41}
Step: 287, {'loss': 0.2714, 'learning_rate': 1.7717546362339515e-06, 'epoch': 0.41}
Step: 288, {'loss': 0.2229, 'learning_rate': 1.767475035663338e-06, 'epoch': 0.41}
Step: 289, {'loss': 0.2364, 'learning_rate': 1.7631954350927249e-06, 'epoch': 0.41}
Step: 290, {'loss': 0.2521, 'learning_rate': 1.7589158345221114e-06, 'epoch': 0.41}
Step: 291, {'loss': 0.2377, 'learning_rate': 1.7546362339514979e-06, 'epoch': 0.42}
Step: 292, {'loss': 0.2296, 'learning_rate': 1.7503566333808844e-06, 'epoch': 0.42}
Step: 293, {'loss': 0.2785, 'learning_rate': 1.7460770328102713e-06, 'epoch': 0.42}
Step: 294, {'loss': 0.2334, 'learning_rate': 1.7417974322396578e-06, 'epoch': 0.42}
Step: 295, {'loss': 0.2934, 'learning_rate': 1.7375178316690443e-06, 'epoch': 0.42}
Step: 296, {'loss': 0.257, 'learning_rate': 1.7332382310984308e-06, 'epoch': 0.42}
Step: 297, {'loss': 0.2407, 'learning_rate': 1.7289586305278175e-06, 'epoch': 0.42}
Step: 298, {'loss': 0.2356, 'learning_rate': 1.724679029957204e-06, 'epoch': 0.43}
Step: 299, {'loss': 0.2521, 'learning_rate': 1.7203994293865905e-06, 'epoch': 0.43}
Step: 300, {'loss': 0.2647, 'learning_rate': 1.716119828815977e-06, 'epoch': 0.43}
Step: 301, {'loss': 0.229, 'learning_rate': 1.711840228245364e-06, 'epoch': 0.43}
Step: 302, {'loss': 0.232, 'learning_rate': 1.7075606276747504e-06, 'epoch': 0.43}
Step: 303, {'loss': 0.2184, 'learning_rate': 1.703281027104137e-06, 'epoch': 0.43}
Step: 304, {'loss': 0.2337, 'learning_rate': 1.6990014265335234e-06, 'epoch': 0.43}
Step: 305, {'loss': 0.2268, 'learning_rate': 1.6947218259629102e-06, 'epoch': 0.44}
Step: 306, {'loss': 0.2673, 'learning_rate': 1.6904422253922969e-06, 'epoch': 0.44}
Step: 307, {'loss': 0.262, 'learning_rate': 1.6861626248216834e-06, 'epoch': 0.44}
Step: 308, {'loss': 0.2799, 'learning_rate': 1.68188302425107e-06, 'epoch': 0.44}
Step: 309, {'loss': 0.2505, 'learning_rate': 1.6776034236804566e-06, 'epoch': 0.44}
Step: 310, {'loss': 0.2885, 'learning_rate': 1.673323823109843e-06, 'epoch': 0.44}
Step: 311, {'loss': 0.2258, 'learning_rate': 1.6690442225392296e-06, 'epoch': 0.44}
Step: 312, {'loss': 0.2536, 'learning_rate': 1.6647646219686165e-06, 'epoch': 0.45}
Step: 313, {'loss': 0.2339, 'learning_rate': 1.660485021398003e-06, 'epoch': 0.45}
Step: 314, {'loss': 0.2921, 'learning_rate': 1.6562054208273895e-06, 'epoch': 0.45}
Step: 315, {'loss': 0.238, 'learning_rate': 1.651925820256776e-06, 'epoch': 0.45}
Step: 316, {'loss': 0.2254, 'learning_rate': 1.6476462196861627e-06, 'epoch': 0.45}
Step: 317, {'loss': 0.2459, 'learning_rate': 1.6433666191155492e-06, 'epoch': 0.45}
Step: 318, {'loss': 0.2087, 'learning_rate': 1.6390870185449357e-06, 'epoch': 0.45}
Step: 319, {'loss': 0.2259, 'learning_rate': 1.6348074179743222e-06, 'epoch': 0.46}
Step: 320, {'loss': 0.2583, 'learning_rate': 1.6305278174037091e-06, 'epoch': 0.46}
Step: 321, {'loss': 0.2352, 'learning_rate': 1.6262482168330956e-06, 'epoch': 0.46}
Step: 322, {'loss': 0.2334, 'learning_rate': 1.6219686162624821e-06, 'epoch': 0.46}
Step: 323, {'loss': 0.2491, 'learning_rate': 1.6176890156918686e-06, 'epoch': 0.46}
Step: 324, {'loss': 0.2098, 'learning_rate': 1.6134094151212556e-06, 'epoch': 0.46}
Step: 325, {'loss': 0.2702, 'learning_rate': 1.609129814550642e-06, 'epoch': 0.46}
Step: 326, {'loss': 0.2383, 'learning_rate': 1.6048502139800286e-06, 'epoch': 0.47}
Step: 327, {'loss': 0.2166, 'learning_rate': 1.600570613409415e-06, 'epoch': 0.47}
Step: 328, {'loss': 0.2479, 'learning_rate': 1.5962910128388018e-06, 'epoch': 0.47}
Step: 329, {'loss': 0.205, 'learning_rate': 1.5920114122681883e-06, 'epoch': 0.47}
Step: 330, {'loss': 0.2771, 'learning_rate': 1.5877318116975748e-06, 'epoch': 0.47}
Step: 331, {'loss': 0.2311, 'learning_rate': 1.5834522111269613e-06, 'epoch': 0.47}
Step: 332, {'loss': 0.2825, 'learning_rate': 1.5791726105563482e-06, 'epoch': 0.47}
Step: 333, {'loss': 0.2551, 'learning_rate': 1.5748930099857347e-06, 'epoch': 0.48}
Step: 334, {'loss': 0.2292, 'learning_rate': 1.5706134094151212e-06, 'epoch': 0.48}
Step: 335, {'loss': 0.2371, 'learning_rate': 1.5663338088445077e-06, 'epoch': 0.48}
Step: 336, {'loss': 0.1943, 'learning_rate': 1.5620542082738946e-06, 'epoch': 0.48}
Step: 337, {'loss': 0.2104, 'learning_rate': 1.5577746077032811e-06, 'epoch': 0.48}
Step: 338, {'loss': 0.22, 'learning_rate': 1.5534950071326676e-06, 'epoch': 0.48}
Step: 339, {'loss': 0.2369, 'learning_rate': 1.5492154065620543e-06, 'epoch': 0.48}
Step: 340, {'loss': 0.2665, 'learning_rate': 1.5449358059914408e-06, 'epoch': 0.49}
Step: 341, {'loss': 0.2643, 'learning_rate': 1.5406562054208273e-06, 'epoch': 0.49}
Step: 342, {'loss': 0.2633, 'learning_rate': 1.5363766048502138e-06, 'epoch': 0.49}
Step: 343, {'loss': 0.216, 'learning_rate': 1.5320970042796008e-06, 'epoch': 0.49}
Step: 344, {'loss': 0.2313, 'learning_rate': 1.5278174037089873e-06, 'epoch': 0.49}
Step: 345, {'loss': 0.2296, 'learning_rate': 1.5235378031383738e-06, 'epoch': 0.49}
Step: 346, {'loss': 0.2433, 'learning_rate': 1.5192582025677603e-06, 'epoch': 0.49}
Step: 347, {'loss': 0.2552, 'learning_rate': 1.5149786019971472e-06, 'epoch': 0.5}
Step: 348, {'loss': 0.2369, 'learning_rate': 1.5106990014265337e-06, 'epoch': 0.5}
Step: 349, {'loss': 0.2185, 'learning_rate': 1.5064194008559202e-06, 'epoch': 0.5}
Step: 350, {'loss': 0.2494, 'learning_rate': 1.5021398002853067e-06, 'epoch': 0.5}
Step: 351, {'loss': 0.2055, 'learning_rate': 1.4978601997146934e-06, 'epoch': 0.5}
Step: 352, {'loss': 0.2335, 'learning_rate': 1.4935805991440799e-06, 'epoch': 0.5}
Step: 353, {'loss': 0.2385, 'learning_rate': 1.4893009985734664e-06, 'epoch': 0.5}
Step: 354, {'loss': 0.2488, 'learning_rate': 1.4850213980028531e-06, 'epoch': 0.5}
Step: 355, {'loss': 0.2821, 'learning_rate': 1.4807417974322396e-06, 'epoch': 0.51}
Step: 356, {'loss': 0.2236, 'learning_rate': 1.4764621968616263e-06, 'epoch': 0.51}
Step: 357, {'loss': 0.2659, 'learning_rate': 1.4721825962910128e-06, 'epoch': 0.51}
Step: 358, {'loss': 0.218, 'learning_rate': 1.4679029957203995e-06, 'epoch': 0.51}
Step: 359, {'loss': 0.2373, 'learning_rate': 1.463623395149786e-06, 'epoch': 0.51}
Step: 360, {'loss': 0.223, 'learning_rate': 1.4593437945791727e-06, 'epoch': 0.51}
Step: 361, {'loss': 0.2299, 'learning_rate': 1.4550641940085592e-06, 'epoch': 0.51}
Step: 362, {'loss': 0.2414, 'learning_rate': 1.4507845934379457e-06, 'epoch': 0.52}
Step: 363, {'loss': 0.2229, 'learning_rate': 1.4465049928673322e-06, 'epoch': 0.52}
Step: 364, {'loss': 0.2084, 'learning_rate': 1.442225392296719e-06, 'epoch': 0.52}
Step: 365, {'loss': 0.2702, 'learning_rate': 1.4379457917261055e-06, 'epoch': 0.52}
Step: 366, {'loss': 0.1985, 'learning_rate': 1.4336661911554922e-06, 'epoch': 0.52}
Step: 367, {'loss': 0.2546, 'learning_rate': 1.4293865905848789e-06, 'epoch': 0.52}
Step: 368, {'loss': 0.2186, 'learning_rate': 1.4251069900142654e-06, 'epoch': 0.52}
Step: 369, {'loss': 0.298, 'learning_rate': 1.420827389443652e-06, 'epoch': 0.53}
Step: 370, {'loss': 0.2319, 'learning_rate': 1.4165477888730386e-06, 'epoch': 0.53}
Step: 371, {'loss': 0.2496, 'learning_rate': 1.4122681883024253e-06, 'epoch': 0.53}
Step: 372, {'loss': 0.2372, 'learning_rate': 1.4079885877318118e-06, 'epoch': 0.53}
Step: 373, {'loss': 0.211, 'learning_rate': 1.4037089871611983e-06, 'epoch': 0.53}
Step: 374, {'loss': 0.2569, 'learning_rate': 1.3994293865905848e-06, 'epoch': 0.53}
Step: 375, {'loss': 0.2388, 'learning_rate': 1.3951497860199715e-06, 'epoch': 0.53}
Step: 376, {'loss': 0.2108, 'learning_rate': 1.390870185449358e-06, 'epoch': 0.54}
Step: 377, {'loss': 0.2461, 'learning_rate': 1.3865905848787447e-06, 'epoch': 0.54}
Step: 378, {'loss': 0.2634, 'learning_rate': 1.3823109843081312e-06, 'epoch': 0.54}
Step: 379, {'loss': 0.253, 'learning_rate': 1.378031383737518e-06, 'epoch': 0.54}
Step: 380, {'loss': 0.2185, 'learning_rate': 1.3737517831669044e-06, 'epoch': 0.54}
Step: 381, {'loss': 0.2102, 'learning_rate': 1.3694721825962912e-06, 'epoch': 0.54}
Step: 382, {'loss': 0.2698, 'learning_rate': 1.3651925820256777e-06, 'epoch': 0.54}
Step: 383, {'loss': 0.2316, 'learning_rate': 1.3609129814550644e-06, 'epoch': 0.55}
Step: 384, {'loss': 0.2549, 'learning_rate': 1.3566333808844509e-06, 'epoch': 0.55}
Step: 385, {'loss': 0.2606, 'learning_rate': 1.3523537803138374e-06, 'epoch': 0.55}
Step: 386, {'loss': 0.2584, 'learning_rate': 1.3480741797432239e-06, 'epoch': 0.55}
Step: 387, {'loss': 0.2699, 'learning_rate': 1.3437945791726106e-06, 'epoch': 0.55}
Step: 388, {'loss': 0.2477, 'learning_rate': 1.339514978601997e-06, 'epoch': 0.55}
Step: 389, {'loss': 0.23, 'learning_rate': 1.3352353780313838e-06, 'epoch': 0.55}
Step: 390, {'loss': 0.233, 'learning_rate': 1.3309557774607703e-06, 'epoch': 0.56}
Step: 391, {'loss': 0.2244, 'learning_rate': 1.326676176890157e-06, 'epoch': 0.56}
Step: 392, {'loss': 0.2602, 'learning_rate': 1.3223965763195435e-06, 'epoch': 0.56}
Step: 393, {'loss': 0.2171, 'learning_rate': 1.3181169757489302e-06, 'epoch': 0.56}
Step: 394, {'loss': 0.202, 'learning_rate': 1.3138373751783167e-06, 'epoch': 0.56}
Step: 395, {'loss': 0.2279, 'learning_rate': 1.3095577746077032e-06, 'epoch': 0.56}
Step: 396, {'loss': 0.2664, 'learning_rate': 1.30527817403709e-06, 'epoch': 0.56}
Step: 397, {'loss': 0.2396, 'learning_rate': 1.3009985734664764e-06, 'epoch': 0.57}
Step: 398, {'loss': 0.2719, 'learning_rate': 1.2967189728958631e-06, 'epoch': 0.57}
Step: 399, {'loss': 0.227, 'learning_rate': 1.2924393723252496e-06, 'epoch': 0.57}
Step: 400, {'loss': 0.2527, 'learning_rate': 1.2881597717546363e-06, 'epoch': 0.57}
Step: 401, {'loss': 0.2321, 'learning_rate': 1.2838801711840228e-06, 'epoch': 0.57}
Step: 402, {'loss': 0.2269, 'learning_rate': 1.2796005706134096e-06, 'epoch': 0.57}
Step: 403, {'loss': 0.248, 'learning_rate': 1.275320970042796e-06, 'epoch': 0.57}
Step: 404, {'loss': 0.2163, 'learning_rate': 1.2710413694721828e-06, 'epoch': 0.58}
Step: 405, {'loss': 0.2173, 'learning_rate': 1.2667617689015693e-06, 'epoch': 0.58}
Step: 406, {'loss': 0.2095, 'learning_rate': 1.2624821683309558e-06, 'epoch': 0.58}
Step: 407, {'loss': 0.2243, 'learning_rate': 1.2582025677603423e-06, 'epoch': 0.58}
Step: 408, {'loss': 0.2021, 'learning_rate': 1.253922967189729e-06, 'epoch': 0.58}
Step: 409, {'loss': 0.2724, 'learning_rate': 1.2496433666191155e-06, 'epoch': 0.58}
Step: 410, {'loss': 0.2292, 'learning_rate': 1.2453637660485022e-06, 'epoch': 0.58}
Step: 411, {'loss': 0.2426, 'learning_rate': 1.2410841654778887e-06, 'epoch': 0.59}
Step: 412, {'loss': 0.2025, 'learning_rate': 1.2368045649072754e-06, 'epoch': 0.59}
Step: 413, {'loss': 0.2654, 'learning_rate': 1.232524964336662e-06, 'epoch': 0.59}
Step: 414, {'loss': 0.2567, 'learning_rate': 1.2282453637660486e-06, 'epoch': 0.59}
Step: 415, {'loss': 0.2485, 'learning_rate': 1.2239657631954351e-06, 'epoch': 0.59}
Step: 416, {'loss': 0.2477, 'learning_rate': 1.2196861626248218e-06, 'epoch': 0.59}
Step: 417, {'loss': 0.2264, 'learning_rate': 1.2154065620542083e-06, 'epoch': 0.59}
Step: 418, {'loss': 0.2454, 'learning_rate': 1.2111269614835948e-06, 'epoch': 0.6}
Step: 419, {'loss': 0.2143, 'learning_rate': 1.2068473609129813e-06, 'epoch': 0.6}
Step: 420, {'loss': 0.2368, 'learning_rate': 1.202567760342368e-06, 'epoch': 0.6}
Step: 421, {'loss': 0.2343, 'learning_rate': 1.1982881597717545e-06, 'epoch': 0.6}
Step: 422, {'loss': 0.308, 'learning_rate': 1.1940085592011413e-06, 'epoch': 0.6}
Step: 423, {'loss': 0.2419, 'learning_rate': 1.1897289586305278e-06, 'epoch': 0.6}
Step: 424, {'loss': 0.2055, 'learning_rate': 1.1854493580599145e-06, 'epoch': 0.6}
Step: 425, {'loss': 0.2514, 'learning_rate': 1.181169757489301e-06, 'epoch': 0.61}
Step: 426, {'loss': 0.2416, 'learning_rate': 1.1768901569186877e-06, 'epoch': 0.61}
Step: 427, {'loss': 0.235, 'learning_rate': 1.1726105563480744e-06, 'epoch': 0.61}
Step: 428, {'loss': 0.2541, 'learning_rate': 1.1683309557774609e-06, 'epoch': 0.61}
Step: 429, {'loss': 0.2351, 'learning_rate': 1.1640513552068474e-06, 'epoch': 0.61}
Step: 430, {'loss': 0.2895, 'learning_rate': 1.159771754636234e-06, 'epoch': 0.61}
Step: 431, {'loss': 0.2107, 'learning_rate': 1.1554921540656206e-06, 'epoch': 0.61}
Step: 432, {'loss': 0.258, 'learning_rate': 1.1512125534950071e-06, 'epoch': 0.62}
Step: 433, {'loss': 0.2076, 'learning_rate': 1.1469329529243938e-06, 'epoch': 0.62}
Step: 434, {'loss': 0.2386, 'learning_rate': 1.1426533523537803e-06, 'epoch': 0.62}
Step: 435, {'loss': 0.2252, 'learning_rate': 1.138373751783167e-06, 'epoch': 0.62}
Step: 436, {'loss': 0.2398, 'learning_rate': 1.1340941512125535e-06, 'epoch': 0.62}
Step: 437, {'loss': 0.2488, 'learning_rate': 1.1298145506419402e-06, 'epoch': 0.62}
Step: 438, {'loss': 0.3311, 'learning_rate': 1.1255349500713267e-06, 'epoch': 0.62}
Step: 439, {'loss': 0.2132, 'learning_rate': 1.1212553495007132e-06, 'epoch': 0.63}
Step: 440, {'loss': 0.2262, 'learning_rate': 1.1169757489301e-06, 'epoch': 0.63}
Step: 441, {'loss': 0.2606, 'learning_rate': 1.1126961483594865e-06, 'epoch': 0.63}
Step: 442, {'loss': 0.2117, 'learning_rate': 1.108416547788873e-06, 'epoch': 0.63}
Step: 443, {'loss': 0.1987, 'learning_rate': 1.1041369472182597e-06, 'epoch': 0.63}
Step: 444, {'loss': 0.2464, 'learning_rate': 1.0998573466476462e-06, 'epoch': 0.63}
Step: 445, {'loss': 0.1969, 'learning_rate': 1.0955777460770329e-06, 'epoch': 0.63}
Step: 446, {'loss': 0.2154, 'learning_rate': 1.0912981455064194e-06, 'epoch': 0.64}
Step: 447, {'loss': 0.2778, 'learning_rate': 1.087018544935806e-06, 'epoch': 0.64}
Step: 448, {'loss': 0.2322, 'learning_rate': 1.0827389443651926e-06, 'epoch': 0.64}
Step: 449, {'loss': 0.2612, 'learning_rate': 1.0784593437945793e-06, 'epoch': 0.64}
Step: 450, {'loss': 0.2567, 'learning_rate': 1.0741797432239658e-06, 'epoch': 0.64}
Step: 451, {'loss': 0.2048, 'learning_rate': 1.0699001426533523e-06, 'epoch': 0.64}
Step: 452, {'loss': 0.2219, 'learning_rate': 1.0656205420827388e-06, 'epoch': 0.64}
Step: 453, {'loss': 0.1902, 'learning_rate': 1.0613409415121255e-06, 'epoch': 0.65}
Step: 454, {'loss': 0.2215, 'learning_rate': 1.057061340941512e-06, 'epoch': 0.65}
Step: 455, {'loss': 0.2282, 'learning_rate': 1.0527817403708987e-06, 'epoch': 0.65}
Step: 456, {'loss': 0.2356, 'learning_rate': 1.0485021398002852e-06, 'epoch': 0.65}
Step: 457, {'loss': 0.2353, 'learning_rate': 1.044222539229672e-06, 'epoch': 0.65}
Step: 458, {'loss': 0.2062, 'learning_rate': 1.0399429386590587e-06, 'epoch': 0.65}
Step: 459, {'loss': 0.2303, 'learning_rate': 1.0356633380884452e-06, 'epoch': 0.65}
Step: 460, {'loss': 0.2415, 'learning_rate': 1.0313837375178319e-06, 'epoch': 0.66}
Step: 461, {'loss': 0.2225, 'learning_rate': 1.0271041369472184e-06, 'epoch': 0.66}
Step: 462, {'loss': 0.231, 'learning_rate': 1.0228245363766049e-06, 'epoch': 0.66}
Step: 463, {'loss': 0.2488, 'learning_rate': 1.0185449358059914e-06, 'epoch': 0.66}
Step: 464, {'loss': 0.2345, 'learning_rate': 1.014265335235378e-06, 'epoch': 0.66}
Step: 465, {'loss': 0.2344, 'learning_rate': 1.0099857346647646e-06, 'epoch': 0.66}
Step: 466, {'loss': 0.2643, 'learning_rate': 1.0057061340941513e-06, 'epoch': 0.66}
Step: 467, {'loss': 0.2091, 'learning_rate': 1.0014265335235378e-06, 'epoch': 0.67}
Step: 468, {'loss': 0.2304, 'learning_rate': 9.971469329529245e-07, 'epoch': 0.67}
Step: 469, {'loss': 0.2365, 'learning_rate': 9.92867332382311e-07, 'epoch': 0.67}
Step: 470, {'loss': 0.2165, 'learning_rate': 9.885877318116977e-07, 'epoch': 0.67}
Step: 471, {'loss': 0.2336, 'learning_rate': 9.843081312410842e-07, 'epoch': 0.67}
Step: 472, {'loss': 0.226, 'learning_rate': 9.80028530670471e-07, 'epoch': 0.67}
Step: 473, {'loss': 0.22, 'learning_rate': 9.757489300998574e-07, 'epoch': 0.67}
Step: 474, {'loss': 0.2411, 'learning_rate': 9.71469329529244e-07, 'epoch': 0.68}
Step: 475, {'loss': 0.1914, 'learning_rate': 9.671897289586304e-07, 'epoch': 0.68}
Step: 476, {'loss': 0.2603, 'learning_rate': 9.629101283880171e-07, 'epoch': 0.68}
Step: 477, {'loss': 0.2202, 'learning_rate': 9.586305278174036e-07, 'epoch': 0.68}
Step: 478, {'loss': 0.2064, 'learning_rate': 9.543509272467903e-07, 'epoch': 0.68}
Step: 479, {'loss': 0.2084, 'learning_rate': 9.500713266761768e-07, 'epoch': 0.68}
Step: 480, {'loss': 0.2358, 'learning_rate': 9.457917261055636e-07, 'epoch': 0.68}
Step: 481, {'loss': 0.2498, 'learning_rate': 9.415121255349501e-07, 'epoch': 0.69}
Step: 482, {'loss': 0.2536, 'learning_rate': 9.372325249643367e-07, 'epoch': 0.69}
Step: 483, {'loss': 0.2949, 'learning_rate': 9.329529243937232e-07, 'epoch': 0.69}
Step: 484, {'loss': 0.2247, 'learning_rate': 9.286733238231099e-07, 'epoch': 0.69}
Step: 485, {'loss': 0.2038, 'learning_rate': 9.243937232524964e-07, 'epoch': 0.69}
Step: 486, {'loss': 0.2483, 'learning_rate': 9.201141226818831e-07, 'epoch': 0.69}
Step: 487, {'loss': 0.2155, 'learning_rate': 9.158345221112696e-07, 'epoch': 0.69}
Step: 488, {'loss': 0.2341, 'learning_rate': 9.115549215406562e-07, 'epoch': 0.7}
Step: 489, {'loss': 0.248, 'learning_rate': 9.072753209700429e-07, 'epoch': 0.7}
Step: 490, {'loss': 0.2281, 'learning_rate': 9.029957203994294e-07, 'epoch': 0.7}
Step: 491, {'loss': 0.2157, 'learning_rate': 8.987161198288161e-07, 'epoch': 0.7}
Step: 492, {'loss': 0.2105, 'learning_rate': 8.944365192582026e-07, 'epoch': 0.7}
Step: 493, {'loss': 0.2635, 'learning_rate': 8.901569186875892e-07, 'epoch': 0.7}
Step: 494, {'loss': 0.2143, 'learning_rate': 8.858773181169757e-07, 'epoch': 0.7}
Step: 495, {'loss': 0.1887, 'learning_rate': 8.815977175463624e-07, 'epoch': 0.71}
Step: 496, {'loss': 0.2016, 'learning_rate': 8.773181169757489e-07, 'epoch': 0.71}
Step: 497, {'loss': 0.1947, 'learning_rate': 8.730385164051357e-07, 'epoch': 0.71}
Step: 498, {'loss': 0.2055, 'learning_rate': 8.687589158345222e-07, 'epoch': 0.71}
Step: 499, {'loss': 0.1742, 'learning_rate': 8.644793152639088e-07, 'epoch': 0.71}
Step: 500, {'loss': 0.2505, 'learning_rate': 8.601997146932953e-07, 'epoch': 0.71}
Step: 501, {'loss': 0.2318, 'learning_rate': 8.55920114122682e-07, 'epoch': 0.71}
Step: 502, {'loss': 0.1909, 'learning_rate': 8.516405135520685e-07, 'epoch': 0.72}
Step: 503, {'loss': 0.226, 'learning_rate': 8.473609129814551e-07, 'epoch': 0.72}
Step: 504, {'loss': 0.2333, 'learning_rate': 8.430813124108417e-07, 'epoch': 0.72}
Step: 505, {'loss': 0.2471, 'learning_rate': 8.388017118402283e-07, 'epoch': 0.72}
Step: 506, {'loss': 0.2187, 'learning_rate': 8.345221112696148e-07, 'epoch': 0.72}
Step: 507, {'loss': 0.2241, 'learning_rate': 8.302425106990015e-07, 'epoch': 0.72}
Step: 508, {'loss': 0.1975, 'learning_rate': 8.25962910128388e-07, 'epoch': 0.72}
Step: 509, {'loss': 0.2141, 'learning_rate': 8.216833095577746e-07, 'epoch': 0.73}
Step: 510, {'loss': 0.2041, 'learning_rate': 8.174037089871611e-07, 'epoch': 0.73}
Step: 511, {'loss': 0.2453, 'learning_rate': 8.131241084165478e-07, 'epoch': 0.73}
Step: 512, {'loss': 0.287, 'learning_rate': 8.088445078459343e-07, 'epoch': 0.73}
Step: 513, {'loss': 0.2193, 'learning_rate': 8.04564907275321e-07, 'epoch': 0.73}
Step: 514, {'loss': 0.2214, 'learning_rate': 8.002853067047075e-07, 'epoch': 0.73}
Step: 515, {'loss': 0.221, 'learning_rate': 7.960057061340941e-07, 'epoch': 0.73}
Step: 516, {'loss': 0.2212, 'learning_rate': 7.917261055634806e-07, 'epoch': 0.74}
Step: 517, {'loss': 0.2274, 'learning_rate': 7.874465049928673e-07, 'epoch': 0.74}
Step: 518, {'loss': 0.2226, 'learning_rate': 7.831669044222538e-07, 'epoch': 0.74}
Step: 519, {'loss': 0.2054, 'learning_rate': 7.788873038516406e-07, 'epoch': 0.74}
Step: 520, {'loss': 0.2306, 'learning_rate': 7.746077032810272e-07, 'epoch': 0.74}
Step: 521, {'loss': 0.2223, 'learning_rate': 7.703281027104137e-07, 'epoch': 0.74}
Step: 522, {'loss': 0.2079, 'learning_rate': 7.660485021398004e-07, 'epoch': 0.74}
Step: 523, {'loss': 0.2149, 'learning_rate': 7.617689015691869e-07, 'epoch': 0.75}
Step: 524, {'loss': 0.2466, 'learning_rate': 7.574893009985736e-07, 'epoch': 0.75}
Step: 525, {'loss': 0.2317, 'learning_rate': 7.532097004279601e-07, 'epoch': 0.75}
Step: 526, {'loss': 0.2183, 'learning_rate': 7.489300998573467e-07, 'epoch': 0.75}
Step: 527, {'loss': 0.2587, 'learning_rate': 7.446504992867332e-07, 'epoch': 0.75}
Step: 528, {'loss': 0.2888, 'learning_rate': 7.403708987161198e-07, 'epoch': 0.75}
Step: 529, {'loss': 0.2115, 'learning_rate': 7.360912981455064e-07, 'epoch': 0.75}
Step: 530, {'loss': 0.21, 'learning_rate': 7.31811697574893e-07, 'epoch': 0.76}
Step: 531, {'loss': 0.2455, 'learning_rate': 7.275320970042796e-07, 'epoch': 0.76}
Step: 532, {'loss': 0.2497, 'learning_rate': 7.232524964336661e-07, 'epoch': 0.76}
Step: 533, {'loss': 0.1911, 'learning_rate': 7.189728958630527e-07, 'epoch': 0.76}
Step: 534, {'loss': 0.2307, 'learning_rate': 7.146932952924394e-07, 'epoch': 0.76}
Step: 535, {'loss': 0.2284, 'learning_rate': 7.10413694721826e-07, 'epoch': 0.76}
Step: 536, {'loss': 0.2122, 'learning_rate': 7.061340941512127e-07, 'epoch': 0.76}
Step: 537, {'loss': 0.2505, 'learning_rate': 7.018544935805992e-07, 'epoch': 0.77}
Step: 538, {'loss': 0.2326, 'learning_rate': 6.975748930099858e-07, 'epoch': 0.77}
Step: 539, {'loss': 0.2855, 'learning_rate': 6.932952924393724e-07, 'epoch': 0.77}
Step: 540, {'loss': 0.2364, 'learning_rate': 6.89015691868759e-07, 'epoch': 0.77}
Step: 541, {'loss': 0.2577, 'learning_rate': 6.847360912981456e-07, 'epoch': 0.77}
Step: 542, {'loss': 0.2422, 'learning_rate': 6.804564907275322e-07, 'epoch': 0.77}
Step: 543, {'loss': 0.2193, 'learning_rate': 6.761768901569187e-07, 'epoch': 0.77}
Step: 544, {'loss': 0.2026, 'learning_rate': 6.718972895863053e-07, 'epoch': 0.78}
Step: 545, {'loss': 0.2634, 'learning_rate': 6.676176890156919e-07, 'epoch': 0.78}
Step: 546, {'loss': 0.2273, 'learning_rate': 6.633380884450785e-07, 'epoch': 0.78}
Step: 547, {'loss': 0.2431, 'learning_rate': 6.590584878744651e-07, 'epoch': 0.78}
Step: 548, {'loss': 0.2421, 'learning_rate': 6.547788873038516e-07, 'epoch': 0.78}
Step: 549, {'loss': 0.2211, 'learning_rate': 6.504992867332382e-07, 'epoch': 0.78}
Step: 550, {'loss': 0.2746, 'learning_rate': 6.462196861626248e-07, 'epoch': 0.78}
Step: 551, {'loss': 0.238, 'learning_rate': 6.419400855920114e-07, 'epoch': 0.79}
Step: 552, {'loss': 0.1971, 'learning_rate': 6.37660485021398e-07, 'epoch': 0.79}
Step: 553, {'loss': 0.225, 'learning_rate': 6.333808844507846e-07, 'epoch': 0.79}
Step: 554, {'loss': 0.2453, 'learning_rate': 6.291012838801711e-07, 'epoch': 0.79}
Step: 555, {'loss': 0.2467, 'learning_rate': 6.248216833095577e-07, 'epoch': 0.79}
Step: 556, {'loss': 0.1942, 'learning_rate': 6.205420827389443e-07, 'epoch': 0.79}
Step: 557, {'loss': 0.2768, 'learning_rate': 6.16262482168331e-07, 'epoch': 0.79}
Step: 558, {'loss': 0.2532, 'learning_rate': 6.119828815977176e-07, 'epoch': 0.8}
Step: 559, {'loss': 0.2277, 'learning_rate': 6.077032810271042e-07, 'epoch': 0.8}
Step: 560, {'loss': 0.1954, 'learning_rate': 6.034236804564907e-07, 'epoch': 0.8}
Step: 561, {'loss': 0.2235, 'learning_rate': 5.991440798858773e-07, 'epoch': 0.8}
Step: 562, {'loss': 0.216, 'learning_rate': 5.948644793152639e-07, 'epoch': 0.8}
Step: 563, {'loss': 0.2357, 'learning_rate': 5.905848787446505e-07, 'epoch': 0.8}
Step: 564, {'loss': 0.2019, 'learning_rate': 5.863052781740372e-07, 'epoch': 0.8}
Step: 565, {'loss': 0.2552, 'learning_rate': 5.820256776034237e-07, 'epoch': 0.81}
Step: 566, {'loss': 0.2338, 'learning_rate': 5.777460770328103e-07, 'epoch': 0.81}
Step: 567, {'loss': 0.2612, 'learning_rate': 5.734664764621969e-07, 'epoch': 0.81}
Step: 568, {'loss': 0.2105, 'learning_rate': 5.691868758915835e-07, 'epoch': 0.81}
Step: 569, {'loss': 0.2148, 'learning_rate': 5.649072753209701e-07, 'epoch': 0.81}
Step: 570, {'loss': 0.246, 'learning_rate': 5.606276747503566e-07, 'epoch': 0.81}
Step: 571, {'loss': 0.2212, 'learning_rate': 5.563480741797432e-07, 'epoch': 0.81}
Step: 572, {'loss': 0.2325, 'learning_rate': 5.520684736091298e-07, 'epoch': 0.82}
Step: 573, {'loss': 0.25, 'learning_rate': 5.477888730385164e-07, 'epoch': 0.82}
Step: 574, {'loss': 0.1999, 'learning_rate': 5.43509272467903e-07, 'epoch': 0.82}
Step: 575, {'loss': 0.2283, 'learning_rate': 5.392296718972897e-07, 'epoch': 0.82}
Step: 576, {'loss': 0.228, 'learning_rate': 5.349500713266762e-07, 'epoch': 0.82}
Step: 577, {'loss': 0.2318, 'learning_rate': 5.306704707560628e-07, 'epoch': 0.82}
Step: 578, {'loss': 0.2191, 'learning_rate': 5.263908701854494e-07, 'epoch': 0.82}
Step: 579, {'loss': 0.2555, 'learning_rate': 5.22111269614836e-07, 'epoch': 0.83}
Step: 580, {'loss': 0.2364, 'learning_rate': 5.178316690442226e-07, 'epoch': 0.83}
Step: 581, {'loss': 0.2106, 'learning_rate': 5.135520684736092e-07, 'epoch': 0.83}
Step: 582, {'loss': 0.2181, 'learning_rate': 5.092724679029957e-07, 'epoch': 0.83}
Step: 583, {'loss': 0.2218, 'learning_rate': 5.049928673323823e-07, 'epoch': 0.83}
Step: 584, {'loss': 0.2237, 'learning_rate': 5.007132667617689e-07, 'epoch': 0.83}
Step: 585, {'loss': 0.2486, 'learning_rate': 4.964336661911555e-07, 'epoch': 0.83}
Step: 586, {'loss': 0.2426, 'learning_rate': 4.921540656205421e-07, 'epoch': 0.84}
Step: 587, {'loss': 0.1788, 'learning_rate': 4.878744650499287e-07, 'epoch': 0.84}
Step: 588, {'loss': 0.2445, 'learning_rate': 4.835948644793152e-07, 'epoch': 0.84}
Step: 589, {'loss': 0.241, 'learning_rate': 4.793152639087018e-07, 'epoch': 0.84}
Step: 590, {'loss': 0.2485, 'learning_rate': 4.750356633380884e-07, 'epoch': 0.84}
Step: 591, {'loss': 0.2124, 'learning_rate': 4.7075606276747503e-07, 'epoch': 0.84}
Step: 592, {'loss': 0.2353, 'learning_rate': 4.664764621968616e-07, 'epoch': 0.84}
Step: 593, {'loss': 0.2319, 'learning_rate': 4.621968616262482e-07, 'epoch': 0.85}
Step: 594, {'loss': 0.2372, 'learning_rate': 4.579172610556348e-07, 'epoch': 0.85}
Step: 595, {'loss': 0.2167, 'learning_rate': 4.5363766048502145e-07, 'epoch': 0.85}
Step: 596, {'loss': 0.1787, 'learning_rate': 4.4935805991440806e-07, 'epoch': 0.85}
Step: 597, {'loss': 0.2323, 'learning_rate': 4.450784593437946e-07, 'epoch': 0.85}
Step: 598, {'loss': 0.239, 'learning_rate': 4.407988587731812e-07, 'epoch': 0.85}
Step: 599, {'loss': 0.2295, 'learning_rate': 4.365192582025678e-07, 'epoch': 0.85}
Step: 600, {'loss': 0.2204, 'learning_rate': 4.322396576319544e-07, 'epoch': 0.86}
Step: 601, {'loss': 0.2022, 'learning_rate': 4.27960057061341e-07, 'epoch': 0.86}
Step: 602, {'loss': 0.287, 'learning_rate': 4.2368045649072754e-07, 'epoch': 0.86}
Step: 603, {'loss': 0.2286, 'learning_rate': 4.1940085592011414e-07, 'epoch': 0.86}
Step: 604, {'loss': 0.1995, 'learning_rate': 4.1512125534950075e-07, 'epoch': 0.86}
Step: 605, {'loss': 0.2286, 'learning_rate': 4.108416547788873e-07, 'epoch': 0.86}
Step: 606, {'loss': 0.2106, 'learning_rate': 4.065620542082739e-07, 'epoch': 0.86}
Step: 607, {'loss': 0.1962, 'learning_rate': 4.022824536376605e-07, 'epoch': 0.87}
Step: 608, {'loss': 0.2213, 'learning_rate': 3.9800285306704707e-07, 'epoch': 0.87}
Step: 609, {'loss': 0.1919, 'learning_rate': 3.937232524964337e-07, 'epoch': 0.87}
Step: 610, {'loss': 0.2049, 'learning_rate': 3.894436519258203e-07, 'epoch': 0.87}
Step: 611, {'loss': 0.221, 'learning_rate': 3.8516405135520683e-07, 'epoch': 0.87}
Step: 612, {'loss': 0.2022, 'learning_rate': 3.8088445078459344e-07, 'epoch': 0.87}
Step: 613, {'loss': 0.2509, 'learning_rate': 3.7660485021398005e-07, 'epoch': 0.87}
Step: 614, {'loss': 0.2625, 'learning_rate': 3.723252496433666e-07, 'epoch': 0.88}
Step: 615, {'loss': 0.2126, 'learning_rate': 3.680456490727532e-07, 'epoch': 0.88}
Step: 616, {'loss': 0.2338, 'learning_rate': 3.637660485021398e-07, 'epoch': 0.88}
Step: 617, {'loss': 0.2184, 'learning_rate': 3.5948644793152636e-07, 'epoch': 0.88}
Step: 618, {'loss': 0.2219, 'learning_rate': 3.55206847360913e-07, 'epoch': 0.88}
Step: 619, {'loss': 0.2741, 'learning_rate': 3.509272467902996e-07, 'epoch': 0.88}
Step: 620, {'loss': 0.1968, 'learning_rate': 3.466476462196862e-07, 'epoch': 0.88}
Step: 621, {'loss': 0.2356, 'learning_rate': 3.423680456490728e-07, 'epoch': 0.89}
Step: 622, {'loss': 0.1793, 'learning_rate': 3.3808844507845934e-07, 'epoch': 0.89}
Step: 623, {'loss': 0.2337, 'learning_rate': 3.3380884450784595e-07, 'epoch': 0.89}
Step: 624, {'loss': 0.2101, 'learning_rate': 3.2952924393723255e-07, 'epoch': 0.89}
Step: 625, {'loss': 0.2448, 'learning_rate': 3.252496433666191e-07, 'epoch': 0.89}
Step: 626, {'loss': 0.2182, 'learning_rate': 3.209700427960057e-07, 'epoch': 0.89}
Step: 627, {'loss': 0.1963, 'learning_rate': 3.166904422253923e-07, 'epoch': 0.89}
Step: 628, {'loss': 0.2409, 'learning_rate': 3.1241084165477887e-07, 'epoch': 0.9}
Step: 629, {'loss': 0.2415, 'learning_rate': 3.081312410841655e-07, 'epoch': 0.9}
Step: 630, {'loss': 0.2507, 'learning_rate': 3.038516405135521e-07, 'epoch': 0.9}
Step: 631, {'loss': 0.236, 'learning_rate': 2.9957203994293864e-07, 'epoch': 0.9}
Step: 632, {'loss': 0.1923, 'learning_rate': 2.9529243937232524e-07, 'epoch': 0.9}
Step: 633, {'loss': 0.2218, 'learning_rate': 2.9101283880171185e-07, 'epoch': 0.9}
Step: 634, {'loss': 0.2315, 'learning_rate': 2.8673323823109845e-07, 'epoch': 0.9}
Step: 635, {'loss': 0.2387, 'learning_rate': 2.8245363766048506e-07, 'epoch': 0.91}
Step: 636, {'loss': 0.1862, 'learning_rate': 2.781740370898716e-07, 'epoch': 0.91}
Step: 637, {'loss': 0.2202, 'learning_rate': 2.738944365192582e-07, 'epoch': 0.91}
Step: 638, {'loss': 0.2558, 'learning_rate': 2.696148359486448e-07, 'epoch': 0.91}
Step: 639, {'loss': 0.2032, 'learning_rate': 2.653352353780314e-07, 'epoch': 0.91}
Step: 640, {'loss': 0.2266, 'learning_rate': 2.61055634807418e-07, 'epoch': 0.91}
Step: 641, {'loss': 0.1992, 'learning_rate': 2.567760342368046e-07, 'epoch': 0.91}
Step: 642, {'loss': 0.2029, 'learning_rate': 2.5249643366619114e-07, 'epoch': 0.92}
Step: 643, {'loss': 0.2182, 'learning_rate': 2.4821683309557775e-07, 'epoch': 0.92}
Step: 644, {'loss': 0.2038, 'learning_rate': 2.4393723252496436e-07, 'epoch': 0.92}
Step: 645, {'loss': 0.2035, 'learning_rate': 2.396576319543509e-07, 'epoch': 0.92}
Step: 646, {'loss': 0.2168, 'learning_rate': 2.3537803138373752e-07, 'epoch': 0.92}
Step: 647, {'loss': 0.2489, 'learning_rate': 2.310984308131241e-07, 'epoch': 0.92}
Step: 648, {'loss': 0.2173, 'learning_rate': 2.2681883024251073e-07, 'epoch': 0.92}
Step: 649, {'loss': 0.2228, 'learning_rate': 2.225392296718973e-07, 'epoch': 0.93}
Step: 650, {'loss': 0.2179, 'learning_rate': 2.182596291012839e-07, 'epoch': 0.93}
Step: 651, {'loss': 0.2252, 'learning_rate': 2.139800285306705e-07, 'epoch': 0.93}
Step: 652, {'loss': 0.2086, 'learning_rate': 2.0970042796005707e-07, 'epoch': 0.93}
Step: 653, {'loss': 0.2445, 'learning_rate': 2.0542082738944365e-07, 'epoch': 0.93}
Step: 654, {'loss': 0.2172, 'learning_rate': 2.0114122681883026e-07, 'epoch': 0.93}
Step: 655, {'loss': 0.1846, 'learning_rate': 1.9686162624821684e-07, 'epoch': 0.93}
Step: 656, {'loss': 0.2497, 'learning_rate': 1.9258202567760342e-07, 'epoch': 0.94}
Step: 657, {'loss': 0.2324, 'learning_rate': 1.8830242510699002e-07, 'epoch': 0.94}
Step: 658, {'loss': 0.2296, 'learning_rate': 1.840228245363766e-07, 'epoch': 0.94}
Step: 659, {'loss': 0.2059, 'learning_rate': 1.7974322396576318e-07, 'epoch': 0.94}
Step: 660, {'loss': 0.1849, 'learning_rate': 1.754636233951498e-07, 'epoch': 0.94}
Step: 661, {'loss': 0.2162, 'learning_rate': 1.711840228245364e-07, 'epoch': 0.94}
Step: 662, {'loss': 0.2448, 'learning_rate': 1.6690442225392297e-07, 'epoch': 0.94}
Step: 663, {'loss': 0.2228, 'learning_rate': 1.6262482168330955e-07, 'epoch': 0.95}
Step: 664, {'loss': 0.225, 'learning_rate': 1.5834522111269616e-07, 'epoch': 0.95}
Step: 665, {'loss': 0.2185, 'learning_rate': 1.5406562054208274e-07, 'epoch': 0.95}
Step: 666, {'loss': 0.2378, 'learning_rate': 1.4978601997146932e-07, 'epoch': 0.95}
Step: 667, {'loss': 0.2236, 'learning_rate': 1.4550641940085592e-07, 'epoch': 0.95}
Step: 668, {'loss': 0.2626, 'learning_rate': 1.4122681883024253e-07, 'epoch': 0.95}
Step: 669, {'loss': 0.2078, 'learning_rate': 1.369472182596291e-07, 'epoch': 0.95}
Step: 670, {'loss': 0.2076, 'learning_rate': 1.326676176890157e-07, 'epoch': 0.96}
Step: 671, {'loss': 0.1874, 'learning_rate': 1.283880171184023e-07, 'epoch': 0.96}
Step: 672, {'loss': 0.2398, 'learning_rate': 1.2410841654778888e-07, 'epoch': 0.96}
Step: 673, {'loss': 0.2575, 'learning_rate': 1.1982881597717545e-07, 'epoch': 0.96}
Step: 674, {'loss': 0.2269, 'learning_rate': 1.1554921540656205e-07, 'epoch': 0.96}
Step: 675, {'loss': 0.2117, 'learning_rate': 1.1126961483594865e-07, 'epoch': 0.96}
Step: 676, {'loss': 0.226, 'learning_rate': 1.0699001426533525e-07, 'epoch': 0.96}
Step: 677, {'loss': 0.1978, 'learning_rate': 1.0271041369472183e-07, 'epoch': 0.97}
Step: 678, {'loss': 0.2049, 'learning_rate': 9.843081312410842e-08, 'epoch': 0.97}
Step: 679, {'loss': 0.2382, 'learning_rate': 9.415121255349501e-08, 'epoch': 0.97}
Step: 680, {'loss': 0.2699, 'learning_rate': 8.987161198288159e-08, 'epoch': 0.97}
Step: 681, {'loss': 0.2876, 'learning_rate': 8.55920114122682e-08, 'epoch': 0.97}
Step: 682, {'loss': 0.2187, 'learning_rate': 8.131241084165478e-08, 'epoch': 0.97}
Step: 683, {'loss': 0.2188, 'learning_rate': 7.703281027104137e-08, 'epoch': 0.97}
Step: 684, {'loss': 0.2298, 'learning_rate': 7.275320970042796e-08, 'epoch': 0.98}
Step: 685, {'loss': 0.2491, 'learning_rate': 6.847360912981455e-08, 'epoch': 0.98}
Step: 686, {'loss': 0.2526, 'learning_rate': 6.419400855920115e-08, 'epoch': 0.98}
Step: 687, {'loss': 0.2448, 'learning_rate': 5.991440798858773e-08, 'epoch': 0.98}
Step: 688, {'loss': 0.2594, 'learning_rate': 5.563480741797433e-08, 'epoch': 0.98}
Step: 689, {'loss': 0.215, 'learning_rate': 5.135520684736091e-08, 'epoch': 0.98}
Step: 690, {'loss': 0.2306, 'learning_rate': 4.7075606276747506e-08, 'epoch': 0.98}
Step: 691, {'loss': 0.1982, 'learning_rate': 4.27960057061341e-08, 'epoch': 0.99}
Step: 692, {'loss': 0.2065, 'learning_rate': 3.8516405135520685e-08, 'epoch': 0.99}
Step: 693, {'loss': 0.2334, 'learning_rate': 3.423680456490728e-08, 'epoch': 0.99}
Step: 694, {'loss': 0.247, 'learning_rate': 2.9957203994293864e-08, 'epoch': 0.99}
Step: 695, {'loss': 0.2416, 'learning_rate': 2.5677603423680456e-08, 'epoch': 0.99}
Step: 696, {'loss': 0.2183, 'learning_rate': 2.139800285306705e-08, 'epoch': 0.99}
Step: 697, {'loss': 0.2191, 'learning_rate': 1.711840228245364e-08, 'epoch': 0.99}
Step: 698, {'loss': 0.2152, 'learning_rate': 1.2838801711840228e-08, 'epoch': 1.0}
Step: 699, {'loss': 0.2289, 'learning_rate': 8.55920114122682e-09, 'epoch': 1.0}
Step: 700, {'loss': 0.2068, 'learning_rate': 4.27960057061341e-09, 'epoch': 1.0}
Step: 701, {'loss': 0.2518, 'learning_rate': 0.0, 'epoch': 1.0}
Step: 701, {'train_runtime': 3074.4814, 'train_samples_per_second': 3.42, 'train_steps_per_second': 0.228, 'total_flos': 7.128891802858291e+17, 'train_loss': 0.24763091731173506, 'epoch': 1.0}
Step: 701, {'eval_loss': 0.31806182861328125, 'eval_accuracy': 0.19, 'eval_f1': 0.8270462633451957, 'eval_runtime': 31.3183, 'eval_samples_per_second': 9.579, 'eval_steps_per_second': 1.213, 'epoch': 1.0}
Step: 1, {'loss': 0.1608, 'learning_rate': 2.9957203994293868e-06, 'epoch': 0.0}
Step: 2, {'loss': 0.2282, 'learning_rate': 2.9914407988587735e-06, 'epoch': 0.0}
Step: 3, {'loss': 0.2164, 'learning_rate': 2.9871611982881598e-06, 'epoch': 0.0}
Step: 4, {'loss': 0.2361, 'learning_rate': 2.9828815977175465e-06, 'epoch': 0.01}
Step: 5, {'loss': 0.214, 'learning_rate': 2.978601997146933e-06, 'epoch': 0.01}
Step: 6, {'loss': 0.2328, 'learning_rate': 2.97432239657632e-06, 'epoch': 0.01}
Step: 7, {'loss': 0.2221, 'learning_rate': 2.9700427960057062e-06, 'epoch': 0.01}
Step: 8, {'loss': 0.2149, 'learning_rate': 2.965763195435093e-06, 'epoch': 0.01}
Step: 9, {'loss': 0.2083, 'learning_rate': 2.9614835948644792e-06, 'epoch': 0.01}
Step: 10, {'loss': 0.1953, 'learning_rate': 2.957203994293866e-06, 'epoch': 0.01}
Step: 11, {'loss': 0.2245, 'learning_rate': 2.9529243937232526e-06, 'epoch': 0.02}
Step: 12, {'loss': 0.2091, 'learning_rate': 2.948644793152639e-06, 'epoch': 0.02}
Step: 13, {'loss': 0.215, 'learning_rate': 2.9443651925820256e-06, 'epoch': 0.02}
Step: 14, {'loss': 0.2049, 'learning_rate': 2.9400855920114123e-06, 'epoch': 0.02}
Step: 15, {'loss': 0.2241, 'learning_rate': 2.935805991440799e-06, 'epoch': 0.02}
Step: 16, {'loss': 0.2279, 'learning_rate': 2.9315263908701853e-06, 'epoch': 0.02}
Step: 17, {'loss': 0.2545, 'learning_rate': 2.927246790299572e-06, 'epoch': 0.02}
Step: 18, {'loss': 0.2519, 'learning_rate': 2.9229671897289588e-06, 'epoch': 0.03}
Step: 19, {'loss': 0.1952, 'learning_rate': 2.9186875891583455e-06, 'epoch': 0.03}
Step: 20, {'loss': 0.2091, 'learning_rate': 2.9144079885877318e-06, 'epoch': 0.03}
Step: 21, {'loss': 0.2332, 'learning_rate': 2.9101283880171185e-06, 'epoch': 0.03}
Step: 22, {'loss': 0.2409, 'learning_rate': 2.905848787446505e-06, 'epoch': 0.03}
Step: 23, {'loss': 0.2213, 'learning_rate': 2.9015691868758915e-06, 'epoch': 0.03}
Step: 24, {'loss': 0.2057, 'learning_rate': 2.897289586305278e-06, 'epoch': 0.03}
Step: 25, {'loss': 0.2205, 'learning_rate': 2.8930099857346645e-06, 'epoch': 0.04}
Step: 26, {'loss': 0.1942, 'learning_rate': 2.8887303851640516e-06, 'epoch': 0.04}
Step: 27, {'loss': 0.2144, 'learning_rate': 2.884450784593438e-06, 'epoch': 0.04}
Step: 28, {'loss': 0.23, 'learning_rate': 2.8801711840228246e-06, 'epoch': 0.04}
Step: 29, {'loss': 0.1951, 'learning_rate': 2.875891583452211e-06, 'epoch': 0.04}
Step: 30, {'loss': 0.2185, 'learning_rate': 2.871611982881598e-06, 'epoch': 0.04}
Step: 31, {'loss': 0.1774, 'learning_rate': 2.8673323823109843e-06, 'epoch': 0.04}
Step: 32, {'loss': 0.2213, 'learning_rate': 2.863052781740371e-06, 'epoch': 0.05}
Step: 33, {'loss': 0.2131, 'learning_rate': 2.8587731811697578e-06, 'epoch': 0.05}
Step: 34, {'loss': 0.2355, 'learning_rate': 2.854493580599144e-06, 'epoch': 0.05}
Step: 35, {'loss': 0.2333, 'learning_rate': 2.8502139800285308e-06, 'epoch': 0.05}
Step: 36, {'loss': 0.2098, 'learning_rate': 2.845934379457917e-06, 'epoch': 0.05}
Step: 37, {'loss': 0.2032, 'learning_rate': 2.841654778887304e-06, 'epoch': 0.05}
Step: 38, {'loss': 0.2207, 'learning_rate': 2.8373751783166905e-06, 'epoch': 0.05}
Step: 39, {'loss': 0.226, 'learning_rate': 2.833095577746077e-06, 'epoch': 0.06}
Step: 40, {'loss': 0.2831, 'learning_rate': 2.8288159771754635e-06, 'epoch': 0.06}
Step: 41, {'loss': 0.225, 'learning_rate': 2.8245363766048506e-06, 'epoch': 0.06}
Step: 42, {'loss': 0.2903, 'learning_rate': 2.820256776034237e-06, 'epoch': 0.06}
Step: 43, {'loss': 0.2237, 'learning_rate': 2.8159771754636236e-06, 'epoch': 0.06}
Step: 44, {'loss': 0.2047, 'learning_rate': 2.81169757489301e-06, 'epoch': 0.06}
Step: 45, {'loss': 0.2333, 'learning_rate': 2.8074179743223966e-06, 'epoch': 0.06}
Step: 46, {'loss': 0.2026, 'learning_rate': 2.8031383737517833e-06, 'epoch': 0.07}
Step: 47, {'loss': 0.1672, 'learning_rate': 2.7988587731811696e-06, 'epoch': 0.07}
Step: 48, {'loss': 0.2101, 'learning_rate': 2.7945791726105563e-06, 'epoch': 0.07}
Step: 49, {'loss': 0.2475, 'learning_rate': 2.790299572039943e-06, 'epoch': 0.07}
Step: 50, {'loss': 0.2242, 'learning_rate': 2.7860199714693297e-06, 'epoch': 0.07}
Step: 51, {'loss': 0.2077, 'learning_rate': 2.781740370898716e-06, 'epoch': 0.07}
Step: 52, {'loss': 0.1967, 'learning_rate': 2.7774607703281027e-06, 'epoch': 0.07}
Step: 53, {'loss': 0.2195, 'learning_rate': 2.7731811697574895e-06, 'epoch': 0.08}
Step: 54, {'loss': 0.2347, 'learning_rate': 2.768901569186876e-06, 'epoch': 0.08}
Step: 55, {'loss': 0.2662, 'learning_rate': 2.7646219686162625e-06, 'epoch': 0.08}
Step: 56, {'loss': 0.1968, 'learning_rate': 2.760342368045649e-06, 'epoch': 0.08}
Step: 57, {'loss': 0.1809, 'learning_rate': 2.756062767475036e-06, 'epoch': 0.08}
Step: 58, {'loss': 0.1799, 'learning_rate': 2.751783166904422e-06, 'epoch': 0.08}
Step: 59, {'loss': 0.198, 'learning_rate': 2.747503566333809e-06, 'epoch': 0.08}
Step: 60, {'loss': 0.1893, 'learning_rate': 2.743223965763195e-06, 'epoch': 0.09}
Step: 61, {'loss': 0.1777, 'learning_rate': 2.7389443651925823e-06, 'epoch': 0.09}
Step: 62, {'loss': 0.1976, 'learning_rate': 2.7346647646219686e-06, 'epoch': 0.09}
Step: 63, {'loss': 0.2368, 'learning_rate': 2.7303851640513553e-06, 'epoch': 0.09}
Step: 64, {'loss': 0.2549, 'learning_rate': 2.726105563480742e-06, 'epoch': 0.09}
Step: 65, {'loss': 0.1952, 'learning_rate': 2.7218259629101287e-06, 'epoch': 0.09}
Step: 66, {'loss': 0.2285, 'learning_rate': 2.717546362339515e-06, 'epoch': 0.09}
Step: 67, {'loss': 0.1986, 'learning_rate': 2.7132667617689017e-06, 'epoch': 0.1}
Step: 68, {'loss': 0.1962, 'learning_rate': 2.7089871611982884e-06, 'epoch': 0.1}
Step: 69, {'loss': 0.2221, 'learning_rate': 2.7047075606276747e-06, 'epoch': 0.1}
Step: 70, {'loss': 0.1854, 'learning_rate': 2.7004279600570614e-06, 'epoch': 0.1}
Step: 71, {'loss': 0.191, 'learning_rate': 2.6961483594864477e-06, 'epoch': 0.1}
Step: 72, {'loss': 0.2336, 'learning_rate': 2.691868758915835e-06, 'epoch': 0.1}
Step: 73, {'loss': 0.1991, 'learning_rate': 2.687589158345221e-06, 'epoch': 0.1}
Step: 74, {'loss': 0.2197, 'learning_rate': 2.683309557774608e-06, 'epoch': 0.11}
Step: 75, {'loss': 0.2141, 'learning_rate': 2.679029957203994e-06, 'epoch': 0.11}
Step: 76, {'loss': 0.2746, 'learning_rate': 2.6747503566333813e-06, 'epoch': 0.11}
Step: 77, {'loss': 0.2157, 'learning_rate': 2.6704707560627676e-06, 'epoch': 0.11}
Step: 78, {'loss': 0.2141, 'learning_rate': 2.6661911554921543e-06, 'epoch': 0.11}
Step: 79, {'loss': 0.2173, 'learning_rate': 2.6619115549215406e-06, 'epoch': 0.11}
Step: 80, {'loss': 0.2136, 'learning_rate': 2.6576319543509273e-06, 'epoch': 0.11}
Step: 81, {'loss': 0.2291, 'learning_rate': 2.653352353780314e-06, 'epoch': 0.12}
Step: 82, {'loss': 0.1957, 'learning_rate': 2.6490727532097003e-06, 'epoch': 0.12}
Step: 83, {'loss': 0.2512, 'learning_rate': 2.644793152639087e-06, 'epoch': 0.12}
Step: 84, {'loss': 0.2197, 'learning_rate': 2.6405135520684737e-06, 'epoch': 0.12}
Step: 85, {'loss': 0.1771, 'learning_rate': 2.6362339514978604e-06, 'epoch': 0.12}
Step: 86, {'loss': 0.2217, 'learning_rate': 2.6319543509272467e-06, 'epoch': 0.12}
Step: 87, {'loss': 0.2148, 'learning_rate': 2.6276747503566334e-06, 'epoch': 0.12}
Step: 88, {'loss': 0.2189, 'learning_rate': 2.62339514978602e-06, 'epoch': 0.13}
Step: 89, {'loss': 0.2206, 'learning_rate': 2.6191155492154064e-06, 'epoch': 0.13}
Step: 90, {'loss': 0.235, 'learning_rate': 2.614835948644793e-06, 'epoch': 0.13}
Step: 91, {'loss': 0.2118, 'learning_rate': 2.61055634807418e-06, 'epoch': 0.13}
Step: 92, {'loss': 0.1902, 'learning_rate': 2.6062767475035666e-06, 'epoch': 0.13}
Step: 93, {'loss': 0.1633, 'learning_rate': 2.601997146932953e-06, 'epoch': 0.13}
Step: 94, {'loss': 0.2085, 'learning_rate': 2.5977175463623396e-06, 'epoch': 0.13}
Step: 95, {'loss': 0.1988, 'learning_rate': 2.5934379457917263e-06, 'epoch': 0.14}
Step: 96, {'loss': 0.2017, 'learning_rate': 2.589158345221113e-06, 'epoch': 0.14}
Step: 97, {'loss': 0.1843, 'learning_rate': 2.5848787446504993e-06, 'epoch': 0.14}
Step: 98, {'loss': 0.1951, 'learning_rate': 2.580599144079886e-06, 'epoch': 0.14}
Step: 99, {'loss': 0.222, 'learning_rate': 2.5763195435092727e-06, 'epoch': 0.14}
Step: 100, {'loss': 0.2358, 'learning_rate': 2.572039942938659e-06, 'epoch': 0.14}
Step: 101, {'loss': 0.2017, 'learning_rate': 2.5677603423680457e-06, 'epoch': 0.14}
Step: 102, {'loss': 0.2067, 'learning_rate': 2.563480741797432e-06, 'epoch': 0.15}
Step: 103, {'loss': 0.1851, 'learning_rate': 2.559201141226819e-06, 'epoch': 0.15}
Step: 104, {'loss': 0.2096, 'learning_rate': 2.5549215406562054e-06, 'epoch': 0.15}
Step: 105, {'loss': 0.2319, 'learning_rate': 2.550641940085592e-06, 'epoch': 0.15}
Step: 106, {'loss': 0.1983, 'learning_rate': 2.5463623395149784e-06, 'epoch': 0.15}
Step: 107, {'loss': 0.2108, 'learning_rate': 2.5420827389443655e-06, 'epoch': 0.15}
Step: 108, {'loss': 0.2097, 'learning_rate': 2.537803138373752e-06, 'epoch': 0.15}
Step: 109, {'loss': 0.2364, 'learning_rate': 2.5335235378031385e-06, 'epoch': 0.16}
Step: 110, {'loss': 0.1857, 'learning_rate': 2.529243937232525e-06, 'epoch': 0.16}
Step: 111, {'loss': 0.2219, 'learning_rate': 2.5249643366619115e-06, 'epoch': 0.16}
Step: 112, {'loss': 0.2081, 'learning_rate': 2.5206847360912983e-06, 'epoch': 0.16}
Step: 113, {'loss': 0.2195, 'learning_rate': 2.5164051355206845e-06, 'epoch': 0.16}
Step: 114, {'loss': 0.2008, 'learning_rate': 2.5121255349500713e-06, 'epoch': 0.16}
Step: 115, {'loss': 0.1758, 'learning_rate': 2.507845934379458e-06, 'epoch': 0.16}
Step: 116, {'loss': 0.2134, 'learning_rate': 2.5035663338088447e-06, 'epoch': 0.17}
Step: 117, {'loss': 0.2314, 'learning_rate': 2.499286733238231e-06, 'epoch': 0.17}
Step: 118, {'loss': 0.2049, 'learning_rate': 2.4950071326676177e-06, 'epoch': 0.17}
Step: 119, {'loss': 0.2046, 'learning_rate': 2.4907275320970044e-06, 'epoch': 0.17}
Step: 120, {'loss': 0.2142, 'learning_rate': 2.486447931526391e-06, 'epoch': 0.17}
Step: 121, {'loss': 0.1938, 'learning_rate': 2.4821683309557774e-06, 'epoch': 0.17}
Step: 122, {'loss': 0.1889, 'learning_rate': 2.477888730385164e-06, 'epoch': 0.17}
Step: 123, {'loss': 0.1881, 'learning_rate': 2.473609129814551e-06, 'epoch': 0.18}
Step: 124, {'loss': 0.2019, 'learning_rate': 2.469329529243937e-06, 'epoch': 0.18}
Step: 125, {'loss': 0.188, 'learning_rate': 2.465049928673324e-06, 'epoch': 0.18}
Step: 126, {'loss': 0.1863, 'learning_rate': 2.4607703281027105e-06, 'epoch': 0.18}
Step: 127, {'loss': 0.1836, 'learning_rate': 2.4564907275320972e-06, 'epoch': 0.18}
Step: 128, {'loss': 0.1995, 'learning_rate': 2.4522111269614835e-06, 'epoch': 0.18}
Step: 129, {'loss': 0.2257, 'learning_rate': 2.4479315263908702e-06, 'epoch': 0.18}
Step: 130, {'loss': 0.2308, 'learning_rate': 2.443651925820257e-06, 'epoch': 0.19}
Step: 131, {'loss': 0.1657, 'learning_rate': 2.4393723252496437e-06, 'epoch': 0.19}
Step: 132, {'loss': 0.1896, 'learning_rate': 2.43509272467903e-06, 'epoch': 0.19}
Step: 133, {'loss': 0.1978, 'learning_rate': 2.4308131241084167e-06, 'epoch': 0.19}
Step: 134, {'loss': 0.1601, 'learning_rate': 2.4265335235378034e-06, 'epoch': 0.19}
Step: 135, {'loss': 0.1821, 'learning_rate': 2.4222539229671897e-06, 'epoch': 0.19}
Step: 136, {'loss': 0.2055, 'learning_rate': 2.4179743223965764e-06, 'epoch': 0.19}
Step: 137, {'loss': 0.1762, 'learning_rate': 2.4136947218259627e-06, 'epoch': 0.2}
Step: 138, {'loss': 0.2108, 'learning_rate': 2.40941512125535e-06, 'epoch': 0.2}
Step: 139, {'loss': 0.174, 'learning_rate': 2.405135520684736e-06, 'epoch': 0.2}
Step: 140, {'loss': 0.2141, 'learning_rate': 2.400855920114123e-06, 'epoch': 0.2}
Step: 141, {'loss': 0.1887, 'learning_rate': 2.396576319543509e-06, 'epoch': 0.2}
Step: 142, {'loss': 0.2088, 'learning_rate': 2.3922967189728962e-06, 'epoch': 0.2}
Step: 143, {'loss': 0.1804, 'learning_rate': 2.3880171184022825e-06, 'epoch': 0.2}
Step: 144, {'loss': 0.1828, 'learning_rate': 2.3837375178316692e-06, 'epoch': 0.21}
Step: 145, {'loss': 0.232, 'learning_rate': 2.3794579172610555e-06, 'epoch': 0.21}
Step: 146, {'loss': 0.1966, 'learning_rate': 2.3751783166904422e-06, 'epoch': 0.21}
Step: 147, {'loss': 0.2253, 'learning_rate': 2.370898716119829e-06, 'epoch': 0.21}
Step: 148, {'loss': 0.2143, 'learning_rate': 2.3666191155492152e-06, 'epoch': 0.21}
Step: 149, {'loss': 0.1958, 'learning_rate': 2.362339514978602e-06, 'epoch': 0.21}
Step: 150, {'loss': 0.1615, 'learning_rate': 2.3580599144079887e-06, 'epoch': 0.21}
Step: 151, {'loss': 0.2482, 'learning_rate': 2.3537803138373754e-06, 'epoch': 0.22}
Step: 152, {'loss': 0.2, 'learning_rate': 2.3495007132667617e-06, 'epoch': 0.22}
Step: 153, {'loss': 0.2065, 'learning_rate': 2.3452211126961488e-06, 'epoch': 0.22}
Step: 154, {'loss': 0.202, 'learning_rate': 2.340941512125535e-06, 'epoch': 0.22}
Step: 155, {'loss': 0.2093, 'learning_rate': 2.3366619115549218e-06, 'epoch': 0.22}
Step: 156, {'loss': 0.1812, 'learning_rate': 2.332382310984308e-06, 'epoch': 0.22}
Step: 157, {'loss': 0.215, 'learning_rate': 2.3281027104136948e-06, 'epoch': 0.22}
Step: 158, {'loss': 0.2023, 'learning_rate': 2.3238231098430815e-06, 'epoch': 0.23}
Step: 159, {'loss': 0.2023, 'learning_rate': 2.319543509272468e-06, 'epoch': 0.23}
Step: 160, {'loss': 0.1667, 'learning_rate': 2.3152639087018545e-06, 'epoch': 0.23}
Step: 161, {'loss': 0.2062, 'learning_rate': 2.3109843081312412e-06, 'epoch': 0.23}
Step: 162, {'loss': 0.1816, 'learning_rate': 2.306704707560628e-06, 'epoch': 0.23}
Step: 163, {'loss': 0.1937, 'learning_rate': 2.3024251069900142e-06, 'epoch': 0.23}
Step: 164, {'loss': 0.1854, 'learning_rate': 2.298145506419401e-06, 'epoch': 0.23}
Step: 165, {'loss': 0.223, 'learning_rate': 2.2938659058487876e-06, 'epoch': 0.24}
Step: 166, {'loss': 0.1801, 'learning_rate': 2.2895863052781743e-06, 'epoch': 0.24}
Step: 167, {'loss': 0.1633, 'learning_rate': 2.2853067047075606e-06, 'epoch': 0.24}
Step: 168, {'loss': 0.2146, 'learning_rate': 2.2810271041369473e-06, 'epoch': 0.24}
Step: 169, {'loss': 0.2307, 'learning_rate': 2.276747503566334e-06, 'epoch': 0.24}
Step: 170, {'loss': 0.1767, 'learning_rate': 2.2724679029957203e-06, 'epoch': 0.24}
Step: 171, {'loss': 0.1923, 'learning_rate': 2.268188302425107e-06, 'epoch': 0.24}
Step: 172, {'loss': 0.1847, 'learning_rate': 2.2639087018544933e-06, 'epoch': 0.25}
Step: 173, {'loss': 0.1799, 'learning_rate': 2.2596291012838805e-06, 'epoch': 0.25}
Step: 174, {'loss': 0.2137, 'learning_rate': 2.2553495007132668e-06, 'epoch': 0.25}
Step: 175, {'loss': 0.2936, 'learning_rate': 2.2510699001426535e-06, 'epoch': 0.25}
Step: 176, {'loss': 0.18, 'learning_rate': 2.2467902995720398e-06, 'epoch': 0.25}
Step: 177, {'loss': 0.2004, 'learning_rate': 2.2425106990014265e-06, 'epoch': 0.25}
Step: 178, {'loss': 0.2134, 'learning_rate': 2.238231098430813e-06, 'epoch': 0.25}
Step: 179, {'loss': 0.2045, 'learning_rate': 2.2339514978602e-06, 'epoch': 0.26}
Step: 180, {'loss': 0.2012, 'learning_rate': 2.229671897289586e-06, 'epoch': 0.26}
Step: 181, {'loss': 0.2215, 'learning_rate': 2.225392296718973e-06, 'epoch': 0.26}
Step: 182, {'loss': 0.2061, 'learning_rate': 2.2211126961483596e-06, 'epoch': 0.26}
Step: 183, {'loss': 0.1919, 'learning_rate': 2.216833095577746e-06, 'epoch': 0.26}
Step: 184, {'loss': 0.2256, 'learning_rate': 2.212553495007133e-06, 'epoch': 0.26}
Step: 185, {'loss': 0.1986, 'learning_rate': 2.2082738944365193e-06, 'epoch': 0.26}
Step: 186, {'loss': 0.1887, 'learning_rate': 2.203994293865906e-06, 'epoch': 0.27}
Step: 187, {'loss': 0.1749, 'learning_rate': 2.1997146932952923e-06, 'epoch': 0.27}
Step: 188, {'loss': 0.1519, 'learning_rate': 2.195435092724679e-06, 'epoch': 0.27}
Step: 189, {'loss': 0.2517, 'learning_rate': 2.1911554921540658e-06, 'epoch': 0.27}
Step: 190, {'loss': 0.1849, 'learning_rate': 2.186875891583452e-06, 'epoch': 0.27}
Step: 191, {'loss': 0.1846, 'learning_rate': 2.1825962910128388e-06, 'epoch': 0.27}
Step: 192, {'loss': 0.2049, 'learning_rate': 2.1783166904422255e-06, 'epoch': 0.27}
Step: 193, {'loss': 0.2043, 'learning_rate': 2.174037089871612e-06, 'epoch': 0.28}
Step: 194, {'loss': 0.1969, 'learning_rate': 2.1697574893009985e-06, 'epoch': 0.28}
Step: 195, {'loss': 0.1944, 'learning_rate': 2.165477888730385e-06, 'epoch': 0.28}
Step: 196, {'loss': 0.1955, 'learning_rate': 2.161198288159772e-06, 'epoch': 0.28}
Step: 197, {'loss': 0.1724, 'learning_rate': 2.1569186875891586e-06, 'epoch': 0.28}
Step: 198, {'loss': 0.2275, 'learning_rate': 2.152639087018545e-06, 'epoch': 0.28}
Step: 199, {'loss': 0.2145, 'learning_rate': 2.1483594864479316e-06, 'epoch': 0.28}
Step: 200, {'loss': 0.1953, 'learning_rate': 2.1440798858773183e-06, 'epoch': 0.29}
Step: 201, {'loss': 0.1837, 'learning_rate': 2.1398002853067046e-06, 'epoch': 0.29}
Step: 202, {'loss': 0.1709, 'learning_rate': 2.1355206847360913e-06, 'epoch': 0.29}
Step: 203, {'loss': 0.1715, 'learning_rate': 2.1312410841654776e-06, 'epoch': 0.29}
Step: 204, {'loss': 0.1819, 'learning_rate': 2.1269614835948647e-06, 'epoch': 0.29}
Step: 205, {'loss': 0.1782, 'learning_rate': 2.122681883024251e-06, 'epoch': 0.29}
Step: 206, {'loss': 0.1924, 'learning_rate': 2.1184022824536377e-06, 'epoch': 0.29}
Step: 207, {'loss': 0.1889, 'learning_rate': 2.114122681883024e-06, 'epoch': 0.3}
Step: 208, {'loss': 0.1676, 'learning_rate': 2.109843081312411e-06, 'epoch': 0.3}
Step: 209, {'loss': 0.2357, 'learning_rate': 2.1055634807417975e-06, 'epoch': 0.3}
Step: 210, {'loss': 0.2373, 'learning_rate': 2.101283880171184e-06, 'epoch': 0.3}
Step: 211, {'loss': 0.1782, 'learning_rate': 2.0970042796005705e-06, 'epoch': 0.3}
Step: 212, {'loss': 0.2439, 'learning_rate': 2.092724679029957e-06, 'epoch': 0.3}
Step: 213, {'loss': 0.2354, 'learning_rate': 2.088445078459344e-06, 'epoch': 0.3}
Step: 214, {'loss': 0.2106, 'learning_rate': 2.08416547788873e-06, 'epoch': 0.31}
Step: 215, {'loss': 0.1929, 'learning_rate': 2.0798858773181173e-06, 'epoch': 0.31}
Step: 216, {'loss': 0.1709, 'learning_rate': 2.0756062767475036e-06, 'epoch': 0.31}
Step: 217, {'loss': 0.194, 'learning_rate': 2.0713266761768903e-06, 'epoch': 0.31}
Step: 218, {'loss': 0.1833, 'learning_rate': 2.0670470756062766e-06, 'epoch': 0.31}
Step: 219, {'loss': 0.1664, 'learning_rate': 2.0627674750356637e-06, 'epoch': 0.31}
Step: 220, {'loss': 0.1999, 'learning_rate': 2.05848787446505e-06, 'epoch': 0.31}
Step: 221, {'loss': 0.2062, 'learning_rate': 2.0542082738944367e-06, 'epoch': 0.32}
Step: 222, {'loss': 0.2226, 'learning_rate': 2.049928673323823e-06, 'epoch': 0.32}
Step: 223, {'loss': 0.2291, 'learning_rate': 2.0456490727532097e-06, 'epoch': 0.32}
Step: 224, {'loss': 0.2213, 'learning_rate': 2.0413694721825964e-06, 'epoch': 0.32}
Step: 225, {'loss': 0.1905, 'learning_rate': 2.0370898716119827e-06, 'epoch': 0.32}
Step: 226, {'loss': 0.2056, 'learning_rate': 2.0328102710413694e-06, 'epoch': 0.32}
Step: 227, {'loss': 0.2206, 'learning_rate': 2.028530670470756e-06, 'epoch': 0.32}
Step: 228, {'loss': 0.1713, 'learning_rate': 2.024251069900143e-06, 'epoch': 0.33}
Step: 229, {'loss': 0.2212, 'learning_rate': 2.019971469329529e-06, 'epoch': 0.33}
Step: 230, {'loss': 0.2, 'learning_rate': 2.015691868758916e-06, 'epoch': 0.33}
Step: 231, {'loss': 0.2092, 'learning_rate': 2.0114122681883026e-06, 'epoch': 0.33}
Step: 232, {'loss': 0.1674, 'learning_rate': 2.0071326676176893e-06, 'epoch': 0.33}
Step: 233, {'loss': 0.2446, 'learning_rate': 2.0028530670470756e-06, 'epoch': 0.33}
Step: 234, {'loss': 0.1773, 'learning_rate': 1.9985734664764623e-06, 'epoch': 0.33}
Step: 235, {'loss': 0.237, 'learning_rate': 1.994293865905849e-06, 'epoch': 0.34}
Step: 236, {'loss': 0.2183, 'learning_rate': 1.9900142653352353e-06, 'epoch': 0.34}
Step: 237, {'loss': 0.179, 'learning_rate': 1.985734664764622e-06, 'epoch': 0.34}
Step: 238, {'loss': 0.2031, 'learning_rate': 1.9814550641940083e-06, 'epoch': 0.34}
Step: 239, {'loss': 0.202, 'learning_rate': 1.9771754636233954e-06, 'epoch': 0.34}
Step: 240, {'loss': 0.1972, 'learning_rate': 1.9728958630527817e-06, 'epoch': 0.34}
Step: 241, {'loss': 0.2076, 'learning_rate': 1.9686162624821684e-06, 'epoch': 0.34}
Step: 242, {'loss': 0.1592, 'learning_rate': 1.9643366619115547e-06, 'epoch': 0.35}
Step: 243, {'loss': 0.2174, 'learning_rate': 1.960057061340942e-06, 'epoch': 0.35}
Step: 244, {'loss': 0.198, 'learning_rate': 1.955777460770328e-06, 'epoch': 0.35}
Step: 245, {'loss': 0.1682, 'learning_rate': 1.951497860199715e-06, 'epoch': 0.35}
Step: 246, {'loss': 0.1921, 'learning_rate': 1.9472182596291016e-06, 'epoch': 0.35}
Step: 247, {'loss': 0.2339, 'learning_rate': 1.942938659058488e-06, 'epoch': 0.35}
Step: 248, {'loss': 0.1984, 'learning_rate': 1.9386590584878746e-06, 'epoch': 0.35}
Step: 249, {'loss': 0.1888, 'learning_rate': 1.934379457917261e-06, 'epoch': 0.36}
Step: 250, {'loss': 0.2133, 'learning_rate': 1.930099857346648e-06, 'epoch': 0.36}
Step: 251, {'loss': 0.2369, 'learning_rate': 1.9258202567760343e-06, 'epoch': 0.36}
Step: 252, {'loss': 0.2058, 'learning_rate': 1.921540656205421e-06, 'epoch': 0.36}
Step: 253, {'loss': 0.1704, 'learning_rate': 1.9172610556348073e-06, 'epoch': 0.36}
Step: 254, {'loss': 0.206, 'learning_rate': 1.912981455064194e-06, 'epoch': 0.36}
Step: 255, {'loss': 0.2013, 'learning_rate': 1.9087018544935807e-06, 'epoch': 0.36}
Step: 256, {'loss': 0.1926, 'learning_rate': 1.9044222539229672e-06, 'epoch': 0.37}
Step: 257, {'loss': 0.2166, 'learning_rate': 1.9001426533523537e-06, 'epoch': 0.37}
Step: 258, {'loss': 0.1576, 'learning_rate': 1.8958630527817406e-06, 'epoch': 0.37}
Step: 259, {'loss': 0.2012, 'learning_rate': 1.8915834522111271e-06, 'epoch': 0.37}
Step: 260, {'loss': 0.2065, 'learning_rate': 1.8873038516405136e-06, 'epoch': 0.37}
Step: 261, {'loss': 0.2351, 'learning_rate': 1.8830242510699001e-06, 'epoch': 0.37}
Step: 262, {'loss': 0.2034, 'learning_rate': 1.8787446504992868e-06, 'epoch': 0.37}
Step: 263, {'loss': 0.1893, 'learning_rate': 1.8744650499286733e-06, 'epoch': 0.38}
Step: 264, {'loss': 0.2425, 'learning_rate': 1.8701854493580598e-06, 'epoch': 0.38}
Step: 265, {'loss': 0.2464, 'learning_rate': 1.8659058487874463e-06, 'epoch': 0.38}
Step: 266, {'loss': 0.2083, 'learning_rate': 1.8616262482168333e-06, 'epoch': 0.38}
Step: 267, {'loss': 0.1662, 'learning_rate': 1.8573466476462198e-06, 'epoch': 0.38}
Step: 268, {'loss': 0.1931, 'learning_rate': 1.8530670470756063e-06, 'epoch': 0.38}
Step: 269, {'loss': 0.2047, 'learning_rate': 1.8487874465049928e-06, 'epoch': 0.38}
Step: 270, {'loss': 0.173, 'learning_rate': 1.8445078459343797e-06, 'epoch': 0.39}
Step: 271, {'loss': 0.186, 'learning_rate': 1.8402282453637662e-06, 'epoch': 0.39}
Step: 272, {'loss': 0.1865, 'learning_rate': 1.8359486447931527e-06, 'epoch': 0.39}
Step: 273, {'loss': 0.1944, 'learning_rate': 1.8316690442225392e-06, 'epoch': 0.39}
Step: 274, {'loss': 0.1959, 'learning_rate': 1.8273894436519259e-06, 'epoch': 0.39}
Step: 275, {'loss': 0.1752, 'learning_rate': 1.8231098430813124e-06, 'epoch': 0.39}
Step: 276, {'loss': 0.1702, 'learning_rate': 1.818830242510699e-06, 'epoch': 0.39}
Step: 277, {'loss': 0.2007, 'learning_rate': 1.8145506419400858e-06, 'epoch': 0.4}
Step: 278, {'loss': 0.1587, 'learning_rate': 1.8102710413694723e-06, 'epoch': 0.4}
Step: 279, {'loss': 0.1971, 'learning_rate': 1.8059914407988588e-06, 'epoch': 0.4}
Step: 280, {'loss': 0.2135, 'learning_rate': 1.8017118402282453e-06, 'epoch': 0.4}
Step: 281, {'loss': 0.1914, 'learning_rate': 1.7974322396576322e-06, 'epoch': 0.4}
Step: 282, {'loss': 0.1681, 'learning_rate': 1.7931526390870187e-06, 'epoch': 0.4}
Step: 283, {'loss': 0.1764, 'learning_rate': 1.7888730385164052e-06, 'epoch': 0.4}
Step: 284, {'loss': 0.2284, 'learning_rate': 1.7845934379457917e-06, 'epoch': 0.41}
Step: 285, {'loss': 0.1904, 'learning_rate': 1.7803138373751785e-06, 'epoch': 0.41}
Step: 286, {'loss': 0.1841, 'learning_rate': 1.776034236804565e-06, 'epoch': 0.41}
Step: 287, {'loss': 0.2325, 'learning_rate': 1.7717546362339515e-06, 'epoch': 0.41}
Step: 288, {'loss': 0.1668, 'learning_rate': 1.767475035663338e-06, 'epoch': 0.41}
Step: 289, {'loss': 0.1875, 'learning_rate': 1.7631954350927249e-06, 'epoch': 0.41}
Step: 290, {'loss': 0.2084, 'learning_rate': 1.7589158345221114e-06, 'epoch': 0.41}
Step: 291, {'loss': 0.1922, 'learning_rate': 1.7546362339514979e-06, 'epoch': 0.42}
Step: 292, {'loss': 0.1761, 'learning_rate': 1.7503566333808844e-06, 'epoch': 0.42}
Step: 293, {'loss': 0.218, 'learning_rate': 1.7460770328102713e-06, 'epoch': 0.42}
Step: 294, {'loss': 0.1882, 'learning_rate': 1.7417974322396578e-06, 'epoch': 0.42}
Step: 295, {'loss': 0.2322, 'learning_rate': 1.7375178316690443e-06, 'epoch': 0.42}
Step: 296, {'loss': 0.2002, 'learning_rate': 1.7332382310984308e-06, 'epoch': 0.42}
Step: 297, {'loss': 0.1924, 'learning_rate': 1.7289586305278175e-06, 'epoch': 0.42}
Step: 298, {'loss': 0.1819, 'learning_rate': 1.724679029957204e-06, 'epoch': 0.43}
Step: 299, {'loss': 0.195, 'learning_rate': 1.7203994293865905e-06, 'epoch': 0.43}
Step: 300, {'loss': 0.2141, 'learning_rate': 1.716119828815977e-06, 'epoch': 0.43}
Step: 301, {'loss': 0.1851, 'learning_rate': 1.711840228245364e-06, 'epoch': 0.43}
Step: 302, {'loss': 0.1754, 'learning_rate': 1.7075606276747504e-06, 'epoch': 0.43}
Step: 303, {'loss': 0.17, 'learning_rate': 1.703281027104137e-06, 'epoch': 0.43}
Step: 304, {'loss': 0.1851, 'learning_rate': 1.6990014265335234e-06, 'epoch': 0.43}
Step: 305, {'loss': 0.179, 'learning_rate': 1.6947218259629102e-06, 'epoch': 0.44}
Step: 306, {'loss': 0.2214, 'learning_rate': 1.6904422253922969e-06, 'epoch': 0.44}
Step: 307, {'loss': 0.2006, 'learning_rate': 1.6861626248216834e-06, 'epoch': 0.44}
Step: 308, {'loss': 0.2196, 'learning_rate': 1.68188302425107e-06, 'epoch': 0.44}
Step: 309, {'loss': 0.212, 'learning_rate': 1.6776034236804566e-06, 'epoch': 0.44}
Step: 310, {'loss': 0.2269, 'learning_rate': 1.673323823109843e-06, 'epoch': 0.44}
Step: 311, {'loss': 0.1679, 'learning_rate': 1.6690442225392296e-06, 'epoch': 0.44}
Step: 312, {'loss': 0.2209, 'learning_rate': 1.6647646219686165e-06, 'epoch': 0.45}
Step: 313, {'loss': 0.182, 'learning_rate': 1.660485021398003e-06, 'epoch': 0.45}
Step: 314, {'loss': 0.2663, 'learning_rate': 1.6562054208273895e-06, 'epoch': 0.45}
Step: 315, {'loss': 0.184, 'learning_rate': 1.651925820256776e-06, 'epoch': 0.45}
Step: 316, {'loss': 0.1677, 'learning_rate': 1.6476462196861627e-06, 'epoch': 0.45}
Step: 317, {'loss': 0.1855, 'learning_rate': 1.6433666191155492e-06, 'epoch': 0.45}
Step: 318, {'loss': 0.1579, 'learning_rate': 1.6390870185449357e-06, 'epoch': 0.45}
Step: 319, {'loss': 0.1737, 'learning_rate': 1.6348074179743222e-06, 'epoch': 0.46}
Step: 320, {'loss': 0.2083, 'learning_rate': 1.6305278174037091e-06, 'epoch': 0.46}
Step: 321, {'loss': 0.1797, 'learning_rate': 1.6262482168330956e-06, 'epoch': 0.46}
Step: 322, {'loss': 0.1922, 'learning_rate': 1.6219686162624821e-06, 'epoch': 0.46}
Step: 323, {'loss': 0.1888, 'learning_rate': 1.6176890156918686e-06, 'epoch': 0.46}
Step: 324, {'loss': 0.158, 'learning_rate': 1.6134094151212556e-06, 'epoch': 0.46}
Step: 325, {'loss': 0.224, 'learning_rate': 1.609129814550642e-06, 'epoch': 0.46}
Step: 326, {'loss': 0.2048, 'learning_rate': 1.6048502139800286e-06, 'epoch': 0.47}
Step: 327, {'loss': 0.1707, 'learning_rate': 1.600570613409415e-06, 'epoch': 0.47}
Step: 328, {'loss': 0.1991, 'learning_rate': 1.5962910128388018e-06, 'epoch': 0.47}
Step: 329, {'loss': 0.1554, 'learning_rate': 1.5920114122681883e-06, 'epoch': 0.47}
Step: 330, {'loss': 0.2309, 'learning_rate': 1.5877318116975748e-06, 'epoch': 0.47}
Step: 331, {'loss': 0.1729, 'learning_rate': 1.5834522111269613e-06, 'epoch': 0.47}
Step: 332, {'loss': 0.2386, 'learning_rate': 1.5791726105563482e-06, 'epoch': 0.47}
Step: 333, {'loss': 0.2295, 'learning_rate': 1.5748930099857347e-06, 'epoch': 0.48}
Step: 334, {'loss': 0.1759, 'learning_rate': 1.5706134094151212e-06, 'epoch': 0.48}
Step: 335, {'loss': 0.1849, 'learning_rate': 1.5663338088445077e-06, 'epoch': 0.48}
Step: 336, {'loss': 0.136, 'learning_rate': 1.5620542082738946e-06, 'epoch': 0.48}
Step: 337, {'loss': 0.1613, 'learning_rate': 1.5577746077032811e-06, 'epoch': 0.48}
Step: 338, {'loss': 0.1558, 'learning_rate': 1.5534950071326676e-06, 'epoch': 0.48}
Step: 339, {'loss': 0.183, 'learning_rate': 1.5492154065620543e-06, 'epoch': 0.48}
Step: 340, {'loss': 0.216, 'learning_rate': 1.5449358059914408e-06, 'epoch': 0.49}
Step: 341, {'loss': 0.2354, 'learning_rate': 1.5406562054208273e-06, 'epoch': 0.49}
Step: 342, {'loss': 0.2132, 'learning_rate': 1.5363766048502138e-06, 'epoch': 0.49}
Step: 343, {'loss': 0.1661, 'learning_rate': 1.5320970042796008e-06, 'epoch': 0.49}
Step: 344, {'loss': 0.1733, 'learning_rate': 1.5278174037089873e-06, 'epoch': 0.49}
Step: 345, {'loss': 0.1799, 'learning_rate': 1.5235378031383738e-06, 'epoch': 0.49}
Step: 346, {'loss': 0.1969, 'learning_rate': 1.5192582025677603e-06, 'epoch': 0.49}
Step: 347, {'loss': 0.2009, 'learning_rate': 1.5149786019971472e-06, 'epoch': 0.5}
Step: 348, {'loss': 0.1794, 'learning_rate': 1.5106990014265337e-06, 'epoch': 0.5}
Step: 349, {'loss': 0.1623, 'learning_rate': 1.5064194008559202e-06, 'epoch': 0.5}
Step: 350, {'loss': 0.1976, 'learning_rate': 1.5021398002853067e-06, 'epoch': 0.5}
Step: 351, {'loss': 0.1514, 'learning_rate': 1.4978601997146934e-06, 'epoch': 0.5}
Step: 352, {'loss': 0.1915, 'learning_rate': 1.4935805991440799e-06, 'epoch': 0.5}
Step: 353, {'loss': 0.1828, 'learning_rate': 1.4893009985734664e-06, 'epoch': 0.5}
Step: 354, {'loss': 0.2027, 'learning_rate': 1.4850213980028531e-06, 'epoch': 0.5}
Step: 355, {'loss': 0.2254, 'learning_rate': 1.4807417974322396e-06, 'epoch': 0.51}
Step: 356, {'loss': 0.1756, 'learning_rate': 1.4764621968616263e-06, 'epoch': 0.51}
Step: 357, {'loss': 0.217, 'learning_rate': 1.4721825962910128e-06, 'epoch': 0.51}
Step: 358, {'loss': 0.1665, 'learning_rate': 1.4679029957203995e-06, 'epoch': 0.51}
Step: 359, {'loss': 0.1951, 'learning_rate': 1.463623395149786e-06, 'epoch': 0.51}
Step: 360, {'loss': 0.1695, 'learning_rate': 1.4593437945791727e-06, 'epoch': 0.51}
Step: 361, {'loss': 0.1809, 'learning_rate': 1.4550641940085592e-06, 'epoch': 0.51}
Step: 362, {'loss': 0.1985, 'learning_rate': 1.4507845934379457e-06, 'epoch': 0.52}
Step: 363, {'loss': 0.185, 'learning_rate': 1.4465049928673322e-06, 'epoch': 0.52}
Step: 364, {'loss': 0.1583, 'learning_rate': 1.442225392296719e-06, 'epoch': 0.52}
Step: 365, {'loss': 0.2323, 'learning_rate': 1.4379457917261055e-06, 'epoch': 0.52}
Step: 366, {'loss': 0.161, 'learning_rate': 1.4336661911554922e-06, 'epoch': 0.52}
Step: 367, {'loss': 0.2035, 'learning_rate': 1.4293865905848789e-06, 'epoch': 0.52}
Step: 368, {'loss': 0.1699, 'learning_rate': 1.4251069900142654e-06, 'epoch': 0.52}
Step: 369, {'loss': 0.2402, 'learning_rate': 1.420827389443652e-06, 'epoch': 0.53}
Step: 370, {'loss': 0.1766, 'learning_rate': 1.4165477888730386e-06, 'epoch': 0.53}
Step: 371, {'loss': 0.209, 'learning_rate': 1.4122681883024253e-06, 'epoch': 0.53}
Step: 372, {'loss': 0.1803, 'learning_rate': 1.4079885877318118e-06, 'epoch': 0.53}
Step: 373, {'loss': 0.1574, 'learning_rate': 1.4037089871611983e-06, 'epoch': 0.53}
Step: 374, {'loss': 0.2127, 'learning_rate': 1.3994293865905848e-06, 'epoch': 0.53}
Step: 375, {'loss': 0.1962, 'learning_rate': 1.3951497860199715e-06, 'epoch': 0.53}
Step: 376, {'loss': 0.1662, 'learning_rate': 1.390870185449358e-06, 'epoch': 0.54}
Step: 377, {'loss': 0.2001, 'learning_rate': 1.3865905848787447e-06, 'epoch': 0.54}
Step: 378, {'loss': 0.2103, 'learning_rate': 1.3823109843081312e-06, 'epoch': 0.54}
Step: 379, {'loss': 0.1957, 'learning_rate': 1.378031383737518e-06, 'epoch': 0.54}
Step: 380, {'loss': 0.1754, 'learning_rate': 1.3737517831669044e-06, 'epoch': 0.54}
Step: 381, {'loss': 0.1579, 'learning_rate': 1.3694721825962912e-06, 'epoch': 0.54}
Step: 382, {'loss': 0.2314, 'learning_rate': 1.3651925820256777e-06, 'epoch': 0.54}
Step: 383, {'loss': 0.1817, 'learning_rate': 1.3609129814550644e-06, 'epoch': 0.55}
Step: 384, {'loss': 0.214, 'learning_rate': 1.3566333808844509e-06, 'epoch': 0.55}
Step: 385, {'loss': 0.2305, 'learning_rate': 1.3523537803138374e-06, 'epoch': 0.55}
Step: 386, {'loss': 0.2057, 'learning_rate': 1.3480741797432239e-06, 'epoch': 0.55}
Step: 387, {'loss': 0.2205, 'learning_rate': 1.3437945791726106e-06, 'epoch': 0.55}
Step: 388, {'loss': 0.2013, 'learning_rate': 1.339514978601997e-06, 'epoch': 0.55}
Step: 389, {'loss': 0.177, 'learning_rate': 1.3352353780313838e-06, 'epoch': 0.55}
Step: 390, {'loss': 0.1906, 'learning_rate': 1.3309557774607703e-06, 'epoch': 0.56}
Step: 391, {'loss': 0.1695, 'learning_rate': 1.326676176890157e-06, 'epoch': 0.56}
Step: 392, {'loss': 0.1917, 'learning_rate': 1.3223965763195435e-06, 'epoch': 0.56}
Step: 393, {'loss': 0.1788, 'learning_rate': 1.3181169757489302e-06, 'epoch': 0.56}
Step: 394, {'loss': 0.1501, 'learning_rate': 1.3138373751783167e-06, 'epoch': 0.56}
Step: 395, {'loss': 0.1893, 'learning_rate': 1.3095577746077032e-06, 'epoch': 0.56}
Step: 396, {'loss': 0.2274, 'learning_rate': 1.30527817403709e-06, 'epoch': 0.56}
Step: 397, {'loss': 0.2026, 'learning_rate': 1.3009985734664764e-06, 'epoch': 0.57}
Step: 398, {'loss': 0.2371, 'learning_rate': 1.2967189728958631e-06, 'epoch': 0.57}
Step: 399, {'loss': 0.1803, 'learning_rate': 1.2924393723252496e-06, 'epoch': 0.57}
Step: 400, {'loss': 0.2243, 'learning_rate': 1.2881597717546363e-06, 'epoch': 0.57}
Step: 401, {'loss': 0.1899, 'learning_rate': 1.2838801711840228e-06, 'epoch': 0.57}
Step: 402, {'loss': 0.1727, 'learning_rate': 1.2796005706134096e-06, 'epoch': 0.57}
Step: 403, {'loss': 0.1978, 'learning_rate': 1.275320970042796e-06, 'epoch': 0.57}
Step: 404, {'loss': 0.1729, 'learning_rate': 1.2710413694721828e-06, 'epoch': 0.58}
Step: 405, {'loss': 0.1734, 'learning_rate': 1.2667617689015693e-06, 'epoch': 0.58}
Step: 406, {'loss': 0.1626, 'learning_rate': 1.2624821683309558e-06, 'epoch': 0.58}
Step: 407, {'loss': 0.1787, 'learning_rate': 1.2582025677603423e-06, 'epoch': 0.58}
Step: 408, {'loss': 0.1617, 'learning_rate': 1.253922967189729e-06, 'epoch': 0.58}
Step: 409, {'loss': 0.2157, 'learning_rate': 1.2496433666191155e-06, 'epoch': 0.58}
Step: 410, {'loss': 0.1926, 'learning_rate': 1.2453637660485022e-06, 'epoch': 0.58}
Step: 411, {'loss': 0.1987, 'learning_rate': 1.2410841654778887e-06, 'epoch': 0.59}
Step: 412, {'loss': 0.1603, 'learning_rate': 1.2368045649072754e-06, 'epoch': 0.59}
Step: 413, {'loss': 0.2253, 'learning_rate': 1.232524964336662e-06, 'epoch': 0.59}
Step: 414, {'loss': 0.2019, 'learning_rate': 1.2282453637660486e-06, 'epoch': 0.59}
Step: 415, {'loss': 0.1976, 'learning_rate': 1.2239657631954351e-06, 'epoch': 0.59}
Step: 416, {'loss': 0.1955, 'learning_rate': 1.2196861626248218e-06, 'epoch': 0.59}
Step: 417, {'loss': 0.1727, 'learning_rate': 1.2154065620542083e-06, 'epoch': 0.59}
Step: 418, {'loss': 0.2032, 'learning_rate': 1.2111269614835948e-06, 'epoch': 0.6}
Step: 419, {'loss': 0.1703, 'learning_rate': 1.2068473609129813e-06, 'epoch': 0.6}
Step: 420, {'loss': 0.1802, 'learning_rate': 1.202567760342368e-06, 'epoch': 0.6}
Step: 421, {'loss': 0.2115, 'learning_rate': 1.1982881597717545e-06, 'epoch': 0.6}
Step: 422, {'loss': 0.2805, 'learning_rate': 1.1940085592011413e-06, 'epoch': 0.6}
Step: 423, {'loss': 0.1993, 'learning_rate': 1.1897289586305278e-06, 'epoch': 0.6}
Step: 424, {'loss': 0.1632, 'learning_rate': 1.1854493580599145e-06, 'epoch': 0.6}
Step: 425, {'loss': 0.2008, 'learning_rate': 1.181169757489301e-06, 'epoch': 0.61}
Step: 426, {'loss': 0.1883, 'learning_rate': 1.1768901569186877e-06, 'epoch': 0.61}
Step: 427, {'loss': 0.1828, 'learning_rate': 1.1726105563480744e-06, 'epoch': 0.61}
Step: 428, {'loss': 0.2055, 'learning_rate': 1.1683309557774609e-06, 'epoch': 0.61}
Step: 429, {'loss': 0.1844, 'learning_rate': 1.1640513552068474e-06, 'epoch': 0.61}
Step: 430, {'loss': 0.246, 'learning_rate': 1.159771754636234e-06, 'epoch': 0.61}
Step: 431, {'loss': 0.1671, 'learning_rate': 1.1554921540656206e-06, 'epoch': 0.61}
Step: 432, {'loss': 0.2093, 'learning_rate': 1.1512125534950071e-06, 'epoch': 0.62}
Step: 433, {'loss': 0.1645, 'learning_rate': 1.1469329529243938e-06, 'epoch': 0.62}
Step: 434, {'loss': 0.1845, 'learning_rate': 1.1426533523537803e-06, 'epoch': 0.62}
Step: 435, {'loss': 0.1692, 'learning_rate': 1.138373751783167e-06, 'epoch': 0.62}
Step: 436, {'loss': 0.1858, 'learning_rate': 1.1340941512125535e-06, 'epoch': 0.62}
Step: 437, {'loss': 0.1989, 'learning_rate': 1.1298145506419402e-06, 'epoch': 0.62}
Step: 438, {'loss': 0.2644, 'learning_rate': 1.1255349500713267e-06, 'epoch': 0.62}
Step: 439, {'loss': 0.1819, 'learning_rate': 1.1212553495007132e-06, 'epoch': 0.63}
Step: 440, {'loss': 0.1864, 'learning_rate': 1.1169757489301e-06, 'epoch': 0.63}
Step: 441, {'loss': 0.2171, 'learning_rate': 1.1126961483594865e-06, 'epoch': 0.63}
Step: 442, {'loss': 0.1695, 'learning_rate': 1.108416547788873e-06, 'epoch': 0.63}
Step: 443, {'loss': 0.1505, 'learning_rate': 1.1041369472182597e-06, 'epoch': 0.63}
Step: 444, {'loss': 0.1951, 'learning_rate': 1.0998573466476462e-06, 'epoch': 0.63}
Step: 445, {'loss': 0.1621, 'learning_rate': 1.0955777460770329e-06, 'epoch': 0.63}
Step: 446, {'loss': 0.1704, 'learning_rate': 1.0912981455064194e-06, 'epoch': 0.64}
Step: 447, {'loss': 0.2277, 'learning_rate': 1.087018544935806e-06, 'epoch': 0.64}
Step: 448, {'loss': 0.1944, 'learning_rate': 1.0827389443651926e-06, 'epoch': 0.64}
Step: 449, {'loss': 0.2062, 'learning_rate': 1.0784593437945793e-06, 'epoch': 0.64}
Step: 450, {'loss': 0.2101, 'learning_rate': 1.0741797432239658e-06, 'epoch': 0.64}
Step: 451, {'loss': 0.1607, 'learning_rate': 1.0699001426533523e-06, 'epoch': 0.64}
Step: 452, {'loss': 0.178, 'learning_rate': 1.0656205420827388e-06, 'epoch': 0.64}
Step: 453, {'loss': 0.1477, 'learning_rate': 1.0613409415121255e-06, 'epoch': 0.65}
Step: 454, {'loss': 0.171, 'learning_rate': 1.057061340941512e-06, 'epoch': 0.65}
Step: 455, {'loss': 0.1795, 'learning_rate': 1.0527817403708987e-06, 'epoch': 0.65}
Step: 456, {'loss': 0.1785, 'learning_rate': 1.0485021398002852e-06, 'epoch': 0.65}
Step: 457, {'loss': 0.1875, 'learning_rate': 1.044222539229672e-06, 'epoch': 0.65}
Step: 458, {'loss': 0.1538, 'learning_rate': 1.0399429386590587e-06, 'epoch': 0.65}
Step: 459, {'loss': 0.192, 'learning_rate': 1.0356633380884452e-06, 'epoch': 0.65}
Step: 460, {'loss': 0.1963, 'learning_rate': 1.0313837375178319e-06, 'epoch': 0.66}
Step: 461, {'loss': 0.1686, 'learning_rate': 1.0271041369472184e-06, 'epoch': 0.66}
Step: 462, {'loss': 0.1852, 'learning_rate': 1.0228245363766049e-06, 'epoch': 0.66}
Step: 463, {'loss': 0.2122, 'learning_rate': 1.0185449358059914e-06, 'epoch': 0.66}
Step: 464, {'loss': 0.1831, 'learning_rate': 1.014265335235378e-06, 'epoch': 0.66}
Step: 465, {'loss': 0.1941, 'learning_rate': 1.0099857346647646e-06, 'epoch': 0.66}
Step: 466, {'loss': 0.2178, 'learning_rate': 1.0057061340941513e-06, 'epoch': 0.66}
Step: 467, {'loss': 0.162, 'learning_rate': 1.0014265335235378e-06, 'epoch': 0.67}
Step: 468, {'loss': 0.1754, 'learning_rate': 9.971469329529245e-07, 'epoch': 0.67}
Step: 469, {'loss': 0.1951, 'learning_rate': 9.92867332382311e-07, 'epoch': 0.67}
Step: 470, {'loss': 0.1694, 'learning_rate': 9.885877318116977e-07, 'epoch': 0.67}
Step: 471, {'loss': 0.1723, 'learning_rate': 9.843081312410842e-07, 'epoch': 0.67}
Step: 472, {'loss': 0.1829, 'learning_rate': 9.80028530670471e-07, 'epoch': 0.67}
Step: 473, {'loss': 0.1732, 'learning_rate': 9.757489300998574e-07, 'epoch': 0.67}
Step: 474, {'loss': 0.2067, 'learning_rate': 9.71469329529244e-07, 'epoch': 0.68}
Step: 475, {'loss': 0.1436, 'learning_rate': 9.671897289586304e-07, 'epoch': 0.68}
Step: 476, {'loss': 0.217, 'learning_rate': 9.629101283880171e-07, 'epoch': 0.68}
Step: 477, {'loss': 0.1787, 'learning_rate': 9.586305278174036e-07, 'epoch': 0.68}
Step: 478, {'loss': 0.1574, 'learning_rate': 9.543509272467903e-07, 'epoch': 0.68}
Step: 479, {'loss': 0.1656, 'learning_rate': 9.500713266761768e-07, 'epoch': 0.68}
Step: 480, {'loss': 0.1913, 'learning_rate': 9.457917261055636e-07, 'epoch': 0.68}
Step: 481, {'loss': 0.2023, 'learning_rate': 9.415121255349501e-07, 'epoch': 0.69}
Step: 482, {'loss': 0.2125, 'learning_rate': 9.372325249643367e-07, 'epoch': 0.69}
Step: 483, {'loss': 0.258, 'learning_rate': 9.329529243937232e-07, 'epoch': 0.69}
Step: 484, {'loss': 0.1698, 'learning_rate': 9.286733238231099e-07, 'epoch': 0.69}
Step: 485, {'loss': 0.1617, 'learning_rate': 9.243937232524964e-07, 'epoch': 0.69}
Step: 486, {'loss': 0.2032, 'learning_rate': 9.201141226818831e-07, 'epoch': 0.69}
Step: 487, {'loss': 0.1669, 'learning_rate': 9.158345221112696e-07, 'epoch': 0.69}
Step: 488, {'loss': 0.1996, 'learning_rate': 9.115549215406562e-07, 'epoch': 0.7}
Step: 489, {'loss': 0.1959, 'learning_rate': 9.072753209700429e-07, 'epoch': 0.7}
Step: 490, {'loss': 0.1811, 'learning_rate': 9.029957203994294e-07, 'epoch': 0.7}
Step: 491, {'loss': 0.1649, 'learning_rate': 8.987161198288161e-07, 'epoch': 0.7}
Step: 492, {'loss': 0.1618, 'learning_rate': 8.944365192582026e-07, 'epoch': 0.7}
Step: 493, {'loss': 0.2096, 'learning_rate': 8.901569186875892e-07, 'epoch': 0.7}
Step: 494, {'loss': 0.1723, 'learning_rate': 8.858773181169757e-07, 'epoch': 0.7}
Step: 495, {'loss': 0.1474, 'learning_rate': 8.815977175463624e-07, 'epoch': 0.71}
Step: 496, {'loss': 0.1552, 'learning_rate': 8.773181169757489e-07, 'epoch': 0.71}
Step: 497, {'loss': 0.1539, 'learning_rate': 8.730385164051357e-07, 'epoch': 0.71}
Step: 498, {'loss': 0.1643, 'learning_rate': 8.687589158345222e-07, 'epoch': 0.71}
Step: 499, {'loss': 0.1434, 'learning_rate': 8.644793152639088e-07, 'epoch': 0.71}
Step: 500, {'loss': 0.2055, 'learning_rate': 8.601997146932953e-07, 'epoch': 0.71}
Step: 501, {'loss': 0.2007, 'learning_rate': 8.55920114122682e-07, 'epoch': 0.71}
Step: 502, {'loss': 0.1455, 'learning_rate': 8.516405135520685e-07, 'epoch': 0.72}
Step: 503, {'loss': 0.1775, 'learning_rate': 8.473609129814551e-07, 'epoch': 0.72}
Step: 504, {'loss': 0.1883, 'learning_rate': 8.430813124108417e-07, 'epoch': 0.72}
Step: 505, {'loss': 0.1892, 'learning_rate': 8.388017118402283e-07, 'epoch': 0.72}
Step: 506, {'loss': 0.1765, 'learning_rate': 8.345221112696148e-07, 'epoch': 0.72}
Step: 507, {'loss': 0.1869, 'learning_rate': 8.302425106990015e-07, 'epoch': 0.72}
Step: 508, {'loss': 0.1615, 'learning_rate': 8.25962910128388e-07, 'epoch': 0.72}
Step: 509, {'loss': 0.1645, 'learning_rate': 8.216833095577746e-07, 'epoch': 0.73}
Step: 510, {'loss': 0.1591, 'learning_rate': 8.174037089871611e-07, 'epoch': 0.73}
Step: 511, {'loss': 0.2165, 'learning_rate': 8.131241084165478e-07, 'epoch': 0.73}
Step: 512, {'loss': 0.2505, 'learning_rate': 8.088445078459343e-07, 'epoch': 0.73}
Step: 513, {'loss': 0.1734, 'learning_rate': 8.04564907275321e-07, 'epoch': 0.73}
Step: 514, {'loss': 0.1752, 'learning_rate': 8.002853067047075e-07, 'epoch': 0.73}
Step: 515, {'loss': 0.1676, 'learning_rate': 7.960057061340941e-07, 'epoch': 0.73}
Step: 516, {'loss': 0.1676, 'learning_rate': 7.917261055634806e-07, 'epoch': 0.74}
Step: 517, {'loss': 0.1867, 'learning_rate': 7.874465049928673e-07, 'epoch': 0.74}
Step: 518, {'loss': 0.1746, 'learning_rate': 7.831669044222538e-07, 'epoch': 0.74}
Step: 519, {'loss': 0.1618, 'learning_rate': 7.788873038516406e-07, 'epoch': 0.74}
Step: 520, {'loss': 0.1869, 'learning_rate': 7.746077032810272e-07, 'epoch': 0.74}
Step: 521, {'loss': 0.1808, 'learning_rate': 7.703281027104137e-07, 'epoch': 0.74}
Step: 522, {'loss': 0.1688, 'learning_rate': 7.660485021398004e-07, 'epoch': 0.74}
Step: 523, {'loss': 0.1673, 'learning_rate': 7.617689015691869e-07, 'epoch': 0.75}
Step: 524, {'loss': 0.2005, 'learning_rate': 7.574893009985736e-07, 'epoch': 0.75}
Step: 525, {'loss': 0.1845, 'learning_rate': 7.532097004279601e-07, 'epoch': 0.75}
Step: 526, {'loss': 0.1682, 'learning_rate': 7.489300998573467e-07, 'epoch': 0.75}
Step: 527, {'loss': 0.2141, 'learning_rate': 7.446504992867332e-07, 'epoch': 0.75}
Step: 528, {'loss': 0.2355, 'learning_rate': 7.403708987161198e-07, 'epoch': 0.75}
Step: 529, {'loss': 0.1698, 'learning_rate': 7.360912981455064e-07, 'epoch': 0.75}
Step: 530, {'loss': 0.1682, 'learning_rate': 7.31811697574893e-07, 'epoch': 0.76}
Step: 531, {'loss': 0.1928, 'learning_rate': 7.275320970042796e-07, 'epoch': 0.76}
Step: 532, {'loss': 0.2093, 'learning_rate': 7.232524964336661e-07, 'epoch': 0.76}
Step: 533, {'loss': 0.151, 'learning_rate': 7.189728958630527e-07, 'epoch': 0.76}
Step: 534, {'loss': 0.1892, 'learning_rate': 7.146932952924394e-07, 'epoch': 0.76}
Step: 535, {'loss': 0.1796, 'learning_rate': 7.10413694721826e-07, 'epoch': 0.76}
Step: 536, {'loss': 0.1716, 'learning_rate': 7.061340941512127e-07, 'epoch': 0.76}
Step: 537, {'loss': 0.2022, 'learning_rate': 7.018544935805992e-07, 'epoch': 0.77}
Step: 538, {'loss': 0.1858, 'learning_rate': 6.975748930099858e-07, 'epoch': 0.77}
Step: 539, {'loss': 0.2433, 'learning_rate': 6.932952924393724e-07, 'epoch': 0.77}
Step: 540, {'loss': 0.1918, 'learning_rate': 6.89015691868759e-07, 'epoch': 0.77}
Step: 541, {'loss': 0.2119, 'learning_rate': 6.847360912981456e-07, 'epoch': 0.77}
Step: 542, {'loss': 0.2141, 'learning_rate': 6.804564907275322e-07, 'epoch': 0.77}
Step: 543, {'loss': 0.1754, 'learning_rate': 6.761768901569187e-07, 'epoch': 0.77}
Step: 544, {'loss': 0.1559, 'learning_rate': 6.718972895863053e-07, 'epoch': 0.78}
Step: 545, {'loss': 0.2234, 'learning_rate': 6.676176890156919e-07, 'epoch': 0.78}
Step: 546, {'loss': 0.1878, 'learning_rate': 6.633380884450785e-07, 'epoch': 0.78}
Step: 547, {'loss': 0.1955, 'learning_rate': 6.590584878744651e-07, 'epoch': 0.78}
Step: 548, {'loss': 0.1987, 'learning_rate': 6.547788873038516e-07, 'epoch': 0.78}
Step: 549, {'loss': 0.1779, 'learning_rate': 6.504992867332382e-07, 'epoch': 0.78}
Step: 550, {'loss': 0.2307, 'learning_rate': 6.462196861626248e-07, 'epoch': 0.78}
Step: 551, {'loss': 0.2021, 'learning_rate': 6.419400855920114e-07, 'epoch': 0.79}
Step: 552, {'loss': 0.1578, 'learning_rate': 6.37660485021398e-07, 'epoch': 0.79}
Step: 553, {'loss': 0.1876, 'learning_rate': 6.333808844507846e-07, 'epoch': 0.79}
Step: 554, {'loss': 0.211, 'learning_rate': 6.291012838801711e-07, 'epoch': 0.79}
Step: 555, {'loss': 0.1973, 'learning_rate': 6.248216833095577e-07, 'epoch': 0.79}
Step: 556, {'loss': 0.1496, 'learning_rate': 6.205420827389443e-07, 'epoch': 0.79}
Step: 557, {'loss': 0.2277, 'learning_rate': 6.16262482168331e-07, 'epoch': 0.79}
Step: 558, {'loss': 0.2107, 'learning_rate': 6.119828815977176e-07, 'epoch': 0.8}
Step: 559, {'loss': 0.1738, 'learning_rate': 6.077032810271042e-07, 'epoch': 0.8}
Step: 560, {'loss': 0.1543, 'learning_rate': 6.034236804564907e-07, 'epoch': 0.8}
Step: 561, {'loss': 0.1764, 'learning_rate': 5.991440798858773e-07, 'epoch': 0.8}
Step: 562, {'loss': 0.1812, 'learning_rate': 5.948644793152639e-07, 'epoch': 0.8}
Step: 563, {'loss': 0.1988, 'learning_rate': 5.905848787446505e-07, 'epoch': 0.8}
Step: 564, {'loss': 0.1671, 'learning_rate': 5.863052781740372e-07, 'epoch': 0.8}
Step: 565, {'loss': 0.2109, 'learning_rate': 5.820256776034237e-07, 'epoch': 0.81}
Step: 566, {'loss': 0.186, 'learning_rate': 5.777460770328103e-07, 'epoch': 0.81}
Step: 567, {'loss': 0.2167, 'learning_rate': 5.734664764621969e-07, 'epoch': 0.81}
Step: 568, {'loss': 0.1757, 'learning_rate': 5.691868758915835e-07, 'epoch': 0.81}
Step: 569, {'loss': 0.1703, 'learning_rate': 5.649072753209701e-07, 'epoch': 0.81}
Step: 570, {'loss': 0.2138, 'learning_rate': 5.606276747503566e-07, 'epoch': 0.81}
Step: 571, {'loss': 0.1814, 'learning_rate': 5.563480741797432e-07, 'epoch': 0.81}
Step: 572, {'loss': 0.1887, 'learning_rate': 5.520684736091298e-07, 'epoch': 0.82}
Step: 573, {'loss': 0.2151, 'learning_rate': 5.477888730385164e-07, 'epoch': 0.82}
Step: 574, {'loss': 0.1571, 'learning_rate': 5.43509272467903e-07, 'epoch': 0.82}
Step: 575, {'loss': 0.1856, 'learning_rate': 5.392296718972897e-07, 'epoch': 0.82}
Step: 576, {'loss': 0.1823, 'learning_rate': 5.349500713266762e-07, 'epoch': 0.82}
Step: 577, {'loss': 0.1819, 'learning_rate': 5.306704707560628e-07, 'epoch': 0.82}
Step: 578, {'loss': 0.1774, 'learning_rate': 5.263908701854494e-07, 'epoch': 0.82}
Step: 579, {'loss': 0.2093, 'learning_rate': 5.22111269614836e-07, 'epoch': 0.83}
Step: 580, {'loss': 0.199, 'learning_rate': 5.178316690442226e-07, 'epoch': 0.83}
Step: 581, {'loss': 0.1748, 'learning_rate': 5.135520684736092e-07, 'epoch': 0.83}
Step: 582, {'loss': 0.1799, 'learning_rate': 5.092724679029957e-07, 'epoch': 0.83}
Step: 583, {'loss': 0.1741, 'learning_rate': 5.049928673323823e-07, 'epoch': 0.83}
Step: 584, {'loss': 0.1869, 'learning_rate': 5.007132667617689e-07, 'epoch': 0.83}
Step: 585, {'loss': 0.2156, 'learning_rate': 4.964336661911555e-07, 'epoch': 0.83}
Step: 586, {'loss': 0.1959, 'learning_rate': 4.921540656205421e-07, 'epoch': 0.84}
Step: 587, {'loss': 0.1417, 'learning_rate': 4.878744650499287e-07, 'epoch': 0.84}
Step: 588, {'loss': 0.2093, 'learning_rate': 4.835948644793152e-07, 'epoch': 0.84}
Step: 589, {'loss': 0.195, 'learning_rate': 4.793152639087018e-07, 'epoch': 0.84}
Step: 590, {'loss': 0.2121, 'learning_rate': 4.750356633380884e-07, 'epoch': 0.84}
Step: 591, {'loss': 0.1731, 'learning_rate': 4.7075606276747503e-07, 'epoch': 0.84}
Step: 592, {'loss': 0.1906, 'learning_rate': 4.664764621968616e-07, 'epoch': 0.84}
Step: 593, {'loss': 0.1854, 'learning_rate': 4.621968616262482e-07, 'epoch': 0.85}
Step: 594, {'loss': 0.1813, 'learning_rate': 4.579172610556348e-07, 'epoch': 0.85}
Step: 595, {'loss': 0.1798, 'learning_rate': 4.5363766048502145e-07, 'epoch': 0.85}
Step: 596, {'loss': 0.133, 'learning_rate': 4.4935805991440806e-07, 'epoch': 0.85}
Step: 597, {'loss': 0.1944, 'learning_rate': 4.450784593437946e-07, 'epoch': 0.85}
Step: 598, {'loss': 0.1965, 'learning_rate': 4.407988587731812e-07, 'epoch': 0.85}
Step: 599, {'loss': 0.1933, 'learning_rate': 4.365192582025678e-07, 'epoch': 0.85}
Step: 600, {'loss': 0.1724, 'learning_rate': 4.322396576319544e-07, 'epoch': 0.86}
Step: 601, {'loss': 0.1631, 'learning_rate': 4.27960057061341e-07, 'epoch': 0.86}
Step: 602, {'loss': 0.2443, 'learning_rate': 4.2368045649072754e-07, 'epoch': 0.86}
Step: 603, {'loss': 0.1782, 'learning_rate': 4.1940085592011414e-07, 'epoch': 0.86}
Step: 604, {'loss': 0.1595, 'learning_rate': 4.1512125534950075e-07, 'epoch': 0.86}
Step: 605, {'loss': 0.1857, 'learning_rate': 4.108416547788873e-07, 'epoch': 0.86}
Step: 606, {'loss': 0.1712, 'learning_rate': 4.065620542082739e-07, 'epoch': 0.86}
Step: 607, {'loss': 0.148, 'learning_rate': 4.022824536376605e-07, 'epoch': 0.87}
Step: 608, {'loss': 0.1734, 'learning_rate': 3.9800285306704707e-07, 'epoch': 0.87}
Step: 609, {'loss': 0.1479, 'learning_rate': 3.937232524964337e-07, 'epoch': 0.87}
Step: 610, {'loss': 0.1591, 'learning_rate': 3.894436519258203e-07, 'epoch': 0.87}
Step: 611, {'loss': 0.1707, 'learning_rate': 3.8516405135520683e-07, 'epoch': 0.87}
Step: 612, {'loss': 0.165, 'learning_rate': 3.8088445078459344e-07, 'epoch': 0.87}
Step: 613, {'loss': 0.1993, 'learning_rate': 3.7660485021398005e-07, 'epoch': 0.87}
Step: 614, {'loss': 0.2207, 'learning_rate': 3.723252496433666e-07, 'epoch': 0.88}
Step: 615, {'loss': 0.1752, 'learning_rate': 3.680456490727532e-07, 'epoch': 0.88}
Step: 616, {'loss': 0.2025, 'learning_rate': 3.637660485021398e-07, 'epoch': 0.88}
Step: 617, {'loss': 0.1746, 'learning_rate': 3.5948644793152636e-07, 'epoch': 0.88}
Step: 618, {'loss': 0.1814, 'learning_rate': 3.55206847360913e-07, 'epoch': 0.88}
Step: 619, {'loss': 0.2263, 'learning_rate': 3.509272467902996e-07, 'epoch': 0.88}
Step: 620, {'loss': 0.152, 'learning_rate': 3.466476462196862e-07, 'epoch': 0.88}
Step: 621, {'loss': 0.1896, 'learning_rate': 3.423680456490728e-07, 'epoch': 0.89}
Step: 622, {'loss': 0.1304, 'learning_rate': 3.3808844507845934e-07, 'epoch': 0.89}
Step: 623, {'loss': 0.1976, 'learning_rate': 3.3380884450784595e-07, 'epoch': 0.89}
Step: 624, {'loss': 0.1621, 'learning_rate': 3.2952924393723255e-07, 'epoch': 0.89}
Step: 625, {'loss': 0.2018, 'learning_rate': 3.252496433666191e-07, 'epoch': 0.89}
Step: 626, {'loss': 0.1693, 'learning_rate': 3.209700427960057e-07, 'epoch': 0.89}
Step: 627, {'loss': 0.1617, 'learning_rate': 3.166904422253923e-07, 'epoch': 0.89}
Step: 628, {'loss': 0.1959, 'learning_rate': 3.1241084165477887e-07, 'epoch': 0.9}
Step: 629, {'loss': 0.1938, 'learning_rate': 3.081312410841655e-07, 'epoch': 0.9}
Step: 630, {'loss': 0.1936, 'learning_rate': 3.038516405135521e-07, 'epoch': 0.9}
Step: 631, {'loss': 0.1902, 'learning_rate': 2.9957203994293864e-07, 'epoch': 0.9}
Step: 632, {'loss': 0.1535, 'learning_rate': 2.9529243937232524e-07, 'epoch': 0.9}
Step: 633, {'loss': 0.1975, 'learning_rate': 2.9101283880171185e-07, 'epoch': 0.9}
Step: 634, {'loss': 0.1844, 'learning_rate': 2.8673323823109845e-07, 'epoch': 0.9}
Step: 635, {'loss': 0.2025, 'learning_rate': 2.8245363766048506e-07, 'epoch': 0.91}
Step: 636, {'loss': 0.1438, 'learning_rate': 2.781740370898716e-07, 'epoch': 0.91}
Step: 637, {'loss': 0.1839, 'learning_rate': 2.738944365192582e-07, 'epoch': 0.91}
Step: 638, {'loss': 0.2078, 'learning_rate': 2.696148359486448e-07, 'epoch': 0.91}
Step: 639, {'loss': 0.1571, 'learning_rate': 2.653352353780314e-07, 'epoch': 0.91}
Step: 640, {'loss': 0.1783, 'learning_rate': 2.61055634807418e-07, 'epoch': 0.91}
Step: 641, {'loss': 0.1702, 'learning_rate': 2.567760342368046e-07, 'epoch': 0.91}
Step: 642, {'loss': 0.1617, 'learning_rate': 2.5249643366619114e-07, 'epoch': 0.92}
Step: 643, {'loss': 0.1765, 'learning_rate': 2.4821683309557775e-07, 'epoch': 0.92}
Step: 644, {'loss': 0.1591, 'learning_rate': 2.4393723252496436e-07, 'epoch': 0.92}
Step: 645, {'loss': 0.1611, 'learning_rate': 2.396576319543509e-07, 'epoch': 0.92}
Step: 646, {'loss': 0.1836, 'learning_rate': 2.3537803138373752e-07, 'epoch': 0.92}
Step: 647, {'loss': 0.2071, 'learning_rate': 2.310984308131241e-07, 'epoch': 0.92}
Step: 648, {'loss': 0.1756, 'learning_rate': 2.2681883024251073e-07, 'epoch': 0.92}
Step: 649, {'loss': 0.1907, 'learning_rate': 2.225392296718973e-07, 'epoch': 0.93}
Step: 650, {'loss': 0.1688, 'learning_rate': 2.182596291012839e-07, 'epoch': 0.93}
Step: 651, {'loss': 0.1927, 'learning_rate': 2.139800285306705e-07, 'epoch': 0.93}
Step: 652, {'loss': 0.1678, 'learning_rate': 2.0970042796005707e-07, 'epoch': 0.93}
Step: 653, {'loss': 0.2107, 'learning_rate': 2.0542082738944365e-07, 'epoch': 0.93}
Step: 654, {'loss': 0.1725, 'learning_rate': 2.0114122681883026e-07, 'epoch': 0.93}
Step: 655, {'loss': 0.1385, 'learning_rate': 1.9686162624821684e-07, 'epoch': 0.93}
Step: 656, {'loss': 0.2006, 'learning_rate': 1.9258202567760342e-07, 'epoch': 0.94}
Step: 657, {'loss': 0.2006, 'learning_rate': 1.8830242510699002e-07, 'epoch': 0.94}
Step: 658, {'loss': 0.1925, 'learning_rate': 1.840228245363766e-07, 'epoch': 0.94}
Step: 659, {'loss': 0.1621, 'learning_rate': 1.7974322396576318e-07, 'epoch': 0.94}
Step: 660, {'loss': 0.1366, 'learning_rate': 1.754636233951498e-07, 'epoch': 0.94}
Step: 661, {'loss': 0.177, 'learning_rate': 1.711840228245364e-07, 'epoch': 0.94}
Step: 662, {'loss': 0.2097, 'learning_rate': 1.6690442225392297e-07, 'epoch': 0.94}
Step: 663, {'loss': 0.1758, 'learning_rate': 1.6262482168330955e-07, 'epoch': 0.95}
Step: 664, {'loss': 0.1895, 'learning_rate': 1.5834522111269616e-07, 'epoch': 0.95}
Step: 665, {'loss': 0.1757, 'learning_rate': 1.5406562054208274e-07, 'epoch': 0.95}
Step: 666, {'loss': 0.199, 'learning_rate': 1.4978601997146932e-07, 'epoch': 0.95}
Step: 667, {'loss': 0.1847, 'learning_rate': 1.4550641940085592e-07, 'epoch': 0.95}
Step: 668, {'loss': 0.2121, 'learning_rate': 1.4122681883024253e-07, 'epoch': 0.95}
Step: 669, {'loss': 0.1663, 'learning_rate': 1.369472182596291e-07, 'epoch': 0.95}
Step: 670, {'loss': 0.1636, 'learning_rate': 1.326676176890157e-07, 'epoch': 0.96}
Step: 671, {'loss': 0.1414, 'learning_rate': 1.283880171184023e-07, 'epoch': 0.96}
Step: 672, {'loss': 0.1989, 'learning_rate': 1.2410841654778888e-07, 'epoch': 0.96}
Step: 673, {'loss': 0.2296, 'learning_rate': 1.1982881597717545e-07, 'epoch': 0.96}
Step: 674, {'loss': 0.1991, 'learning_rate': 1.1554921540656205e-07, 'epoch': 0.96}
Step: 675, {'loss': 0.1695, 'learning_rate': 1.1126961483594865e-07, 'epoch': 0.96}
Step: 676, {'loss': 0.1866, 'learning_rate': 1.0699001426533525e-07, 'epoch': 0.96}
Step: 677, {'loss': 0.1633, 'learning_rate': 1.0271041369472183e-07, 'epoch': 0.97}
Step: 678, {'loss': 0.1663, 'learning_rate': 9.843081312410842e-08, 'epoch': 0.97}
Step: 679, {'loss': 0.2019, 'learning_rate': 9.415121255349501e-08, 'epoch': 0.97}
Step: 680, {'loss': 0.2311, 'learning_rate': 8.987161198288159e-08, 'epoch': 0.97}
Step: 681, {'loss': 0.2482, 'learning_rate': 8.55920114122682e-08, 'epoch': 0.97}
Step: 682, {'loss': 0.1672, 'learning_rate': 8.131241084165478e-08, 'epoch': 0.97}
Step: 683, {'loss': 0.179, 'learning_rate': 7.703281027104137e-08, 'epoch': 0.97}
Step: 684, {'loss': 0.1811, 'learning_rate': 7.275320970042796e-08, 'epoch': 0.98}
Step: 685, {'loss': 0.2043, 'learning_rate': 6.847360912981455e-08, 'epoch': 0.98}
Step: 686, {'loss': 0.212, 'learning_rate': 6.419400855920115e-08, 'epoch': 0.98}
Step: 687, {'loss': 0.2099, 'learning_rate': 5.991440798858773e-08, 'epoch': 0.98}
Step: 688, {'loss': 0.237, 'learning_rate': 5.563480741797433e-08, 'epoch': 0.98}
Step: 689, {'loss': 0.1858, 'learning_rate': 5.135520684736091e-08, 'epoch': 0.98}
Step: 690, {'loss': 0.185, 'learning_rate': 4.7075606276747506e-08, 'epoch': 0.98}
Step: 691, {'loss': 0.1586, 'learning_rate': 4.27960057061341e-08, 'epoch': 0.99}
Step: 692, {'loss': 0.1572, 'learning_rate': 3.8516405135520685e-08, 'epoch': 0.99}
Step: 693, {'loss': 0.1972, 'learning_rate': 3.423680456490728e-08, 'epoch': 0.99}
Step: 694, {'loss': 0.2045, 'learning_rate': 2.9957203994293864e-08, 'epoch': 0.99}
Step: 695, {'loss': 0.1942, 'learning_rate': 2.5677603423680456e-08, 'epoch': 0.99}
Step: 696, {'loss': 0.1861, 'learning_rate': 2.139800285306705e-08, 'epoch': 0.99}
Step: 697, {'loss': 0.1806, 'learning_rate': 1.711840228245364e-08, 'epoch': 0.99}
Step: 698, {'loss': 0.1783, 'learning_rate': 1.2838801711840228e-08, 'epoch': 1.0}
Step: 699, {'loss': 0.1863, 'learning_rate': 8.55920114122682e-09, 'epoch': 1.0}
Step: 700, {'loss': 0.1702, 'learning_rate': 4.27960057061341e-09, 'epoch': 1.0}
Step: 701, {'loss': 0.2139, 'learning_rate': 0.0, 'epoch': 1.0}
Step: 701, {'train_runtime': 3073.4229, 'train_samples_per_second': 3.421, 'train_steps_per_second': 0.228, 'total_flos': 7.128891802858291e+17, 'train_loss': 0.19463367269144588, 'epoch': 1.0}
Step: 701, {'eval_loss': 0.22027897834777832, 'eval_accuracy': 0.3699, 'eval_f1': 0.8787417634967688, 'eval_runtime': 4188.2695, 'eval_samples_per_second': 9.55, 'eval_steps_per_second': 1.194, 'epoch': 1.0}
